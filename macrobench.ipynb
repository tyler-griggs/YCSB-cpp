{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration: Operation sizes (in KB)\n",
    "READ_SIZE_KB = 1   # Size of a single READ operation\n",
    "INSERT_SIZE_KB = 1 # Size of a single INSERT operation\n",
    "INSERT_BATCH_SIZE_KB = 100 # Size of a single INSERT_BATCH operation\n",
    "\n",
    "READ_SIZE_MB = READ_SIZE_KB / 1024\n",
    "INSERT_SIZE_MB = INSERT_SIZE_KB / 1024\n",
    "INSERT_BATCH_SIZE_MB = INSERT_BATCH_SIZE_KB / 1024\n",
    "\n",
    "df = pd.read_csv('logs/client_stats.log')\n",
    "df['relative_time_ms'] = df['timestamp'] - df['timestamp'].min()\n",
    "df['relative_time_s'] = df['relative_time_ms'] / 1000.0\n",
    "\n",
    "df = df.sort_values(by=['client_id', 'op_type', 'timestamp'])\n",
    "df['time_diff_s'] = df.groupby(['client_id', 'op_type'])['relative_time_s'].diff()\n",
    "\n",
    "df['throughput'] = df['count'] / df['time_diff_s']\n",
    "df['throughput_mb_s'] = 0.0\n",
    "\n",
    "df.loc[df['op_type'] == 'READ', 'throughput_mb_s'] = df['throughput'] * READ_SIZE_MB\n",
    "df.loc[df['op_type'] == 'INSERT', 'throughput_mb_s'] = df['throughput'] * INSERT_SIZE_MB\n",
    "df.loc[df['op_type'] == 'INSERT_BATCH', 'throughput_mb_s'] = df['throughput'] * INSERT_BATCH_SIZE_MB\n",
    "\n",
    "df_throughput = df.dropna(subset=['throughput_mb_s'])\n",
    "\n",
    "df_read = df_throughput[df_throughput['op_type'] == 'READ']\n",
    "df_insert = df_throughput[df_throughput['op_type'] == 'INSERT']\n",
    "df_insert_batch = df_throughput[df_throughput['op_type'] == 'INSERT_BATCH']\n",
    "df_queue_latency = df[df['op_type'] == 'QUEUE']\n",
    "df_read_latency = df[df['op_type'] == 'READ']\n",
    "df_insert_latency = df[df['op_type'] == 'INSERT']\n",
    "\n",
    "df_read = df_read.copy()\n",
    "df_read['user_cache_total'] = df_read['user_cache_hits'] + df_read['user_cache_misses']\n",
    "df_read['user_cache_hit_rate'] = np.where(\n",
    "    df_read['user_cache_total'] > 0,\n",
    "    (df_read['user_cache_hits'] / df_read['user_cache_total']) * 100.0,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df_cache_usage = df_read[['relative_time_s', 'client_id', 'user_cache_usage']].dropna()\n",
    "df_cache_usage_pivot = df_cache_usage.pivot_table(\n",
    "    index='relative_time_s',\n",
    "    columns='client_id',\n",
    "    values='user_cache_usage',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Unified color palette (shades of red)\n",
    "colors = [\"#ffc1c1\", \"#fc9598\", \"#ff696e\", \"#f02225\", \"#cf1d20\", \"#ab1619\", \"#851114\", \"#610c0e\"]\n",
    "\n",
    "def plot_memtable_shares(ax, file_path):\n",
    "    client_data = defaultdict(lambda: {'global': [], 'steady': []})\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"wbm\"):\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) == 7 and parts[3] == 'res':\n",
    "                    try:\n",
    "                        _, timestamp, client_id_str, operation, operation_size, current_value_str, steady_or_global = parts\n",
    "                        timestamp = int(timestamp) / 1000\n",
    "                        client_id = int(client_id_str)\n",
    "                        current_value = int(current_value_str) / (1024 * 1024)\n",
    "                        if steady_or_global.strip() in ['steady', 'global']:\n",
    "                            client_data[client_id][steady_or_global.strip()].append((timestamp, current_value))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                elif len(parts) == 8 and parts[3] == 'free':\n",
    "                    try:\n",
    "                        _, timestamp, client_id_str, operation, operation_size, current_value_str, global_str, steady_str = parts\n",
    "                        timestamp = int(timestamp) / 1000\n",
    "                        client_id = int(client_id_str)\n",
    "                        global_value = int(global_str.split(':')[1]) / (1024 * 1024)\n",
    "                        steady_value = int(steady_str.split(':')[1]) / (1024 * 1024)\n",
    "                        client_data[client_id]['global'].append((timestamp, global_value))\n",
    "                        client_data[client_id]['steady'].append((timestamp, steady_value))\n",
    "                    except (ValueError, IndexError):\n",
    "                        continue\n",
    "\n",
    "    start_time = min(\n",
    "        min((timestamps[0][0] for timestamps in client.values() if len(timestamps) > 0), default=float('inf'))\n",
    "        for client in client_data.values()\n",
    "    )\n",
    "\n",
    "    for client_id in client_data:\n",
    "        for key in ['global', 'steady']:\n",
    "            values = client_data[client_id][key]\n",
    "            adjusted_values = [(t - start_time, v) for t, v in values]\n",
    "            client_data[client_id][key] = np.array(adjusted_values)\n",
    "\n",
    "    all_timestamps = sorted(set(\n",
    "        t\n",
    "        for cvals in client_data.values()\n",
    "        for key in ['global', 'steady']\n",
    "        for t, _ in cvals[key]\n",
    "    ))\n",
    "\n",
    "    latest_values = {cid: {'global': 0, 'steady': 0} for cid in client_data}\n",
    "    total_usage_values = []\n",
    "    sum_steady_values = []\n",
    "    sum_global_values = []\n",
    "\n",
    "    for ts in all_timestamps:\n",
    "        total_global = 0\n",
    "        total_steady = 0\n",
    "        total_usage = 0\n",
    "        for cid in client_data:\n",
    "            for key in ['global', 'steady']:\n",
    "                vals = client_data[cid][key]\n",
    "                if len(vals) == 0:\n",
    "                    continue\n",
    "                mask = vals[:, 0] <= ts\n",
    "                if np.any(mask):\n",
    "                    latest_values[cid][key] = vals[mask, 1][-1]\n",
    "            total_client_usage = latest_values[cid]['global'] + latest_values[cid]['steady']\n",
    "            total_usage += total_client_usage\n",
    "            total_global += latest_values[cid]['global']\n",
    "            total_steady += latest_values[cid]['steady']\n",
    "        total_usage_values.append((ts, total_usage))\n",
    "        sum_steady_values.append((ts, total_steady))\n",
    "        sum_global_values.append((ts, total_global))\n",
    "\n",
    "    total_usage_values = np.array(total_usage_values)\n",
    "    sum_steady_values = np.array(sum_steady_values)\n",
    "    sum_global_values = np.array(sum_global_values)\n",
    "\n",
    "    client_total_usage = {}\n",
    "    for cid in client_data:\n",
    "        timestamps = []\n",
    "        usage_values = []\n",
    "        lv = {'global': 0, 'steady': 0}\n",
    "        for ts in all_timestamps:\n",
    "            for key in ['global', 'steady']:\n",
    "                vals = client_data[cid][key]\n",
    "                if len(vals) == 0:\n",
    "                    continue\n",
    "                mask = vals[:, 0] <= ts\n",
    "                if np.any(mask):\n",
    "                    lv[key] = vals[mask, 1][-1]\n",
    "            total_client_usage = lv['global'] + lv['steady']\n",
    "            timestamps.append(ts)\n",
    "            usage_values.append(total_client_usage)\n",
    "        client_total_usage[cid] = np.array([timestamps, usage_values])\n",
    "\n",
    "    for cid, data in client_total_usage.items():\n",
    "        ax.plot(data[0], data[1], label=f\"Client {cid} Total Usage\")\n",
    "\n",
    "    ax.plot(total_usage_values[:, 0], total_usage_values[:, 1], label=\"All Clients Total Usage\", color=\"black\", linewidth=2)\n",
    "    ax.plot(sum_steady_values[:, 0], sum_steady_values[:, 1], label=\"Total Steady Usage\", color=\"green\", linestyle=\"--\", linewidth=2)\n",
    "    ax.plot(sum_global_values[:, 0], sum_global_values[:, 1], label=\"Total Global Usage\", color=\"blue\", linestyle=\":\", linewidth=2)\n",
    "\n",
    "    ax.set_xlabel(\"Time Since Start (s)\")\n",
    "    ax.set_ylabel(\"Usage (MB)\")\n",
    "    ax.set_title(\"Memtable Shares Over Time\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "def plot_database_operations(ax, experiment_start_time, xlim=None):\n",
    "    log_file_path = '/mnt/rocksdb/ycsb-rocksdb-data/LOG'\n",
    "\n",
    "    # Regex patterns\n",
    "    flush_regex = re.compile(\n",
    "        r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\d+ \\[/flush_job\\.cc:\\d+\\] \\[(.*?)\\] \\[JOB \\d+\\] Flush: (\\d+) microseconds, \\d+ cpu microseconds, (\\d+) bytes'\n",
    "    )\n",
    "    l0_stall_pattern = re.compile(r'Stalling writes because we have \\d+ level-0 files rate (\\d+)')\n",
    "    memtable_stall_pattern = re.compile(r'Stalling writes because we have \\d+ immutable memtables.*rate (\\d+)')\n",
    "    pending_compaction_stall_pattern = re.compile(r'Stalling writes because of estimated pending compaction bytes \\d+ rate (\\d+)')\n",
    "    memtable_stop_pattern = re.compile(r'Stopping writes because we have \\d+ immutable memtables.*')\n",
    "    compaction_regex = re.compile(r'.*EVENT_LOG_v1 (.*)$')\n",
    "\n",
    "    def timestamp_to_seconds(timestamp_str):\n",
    "        dt = datetime.strptime(timestamp_str, '%Y/%m/%d-%H:%M:%S.%f')\n",
    "        epoch = datetime(1970, 1, 1)\n",
    "        return (dt - epoch).total_seconds()\n",
    "\n",
    "    def timestamp_to_micros(timestamp_str):\n",
    "        dt = datetime.strptime(timestamp_str, '%Y/%m/%d-%H:%M:%S.%f')\n",
    "        epoch = datetime(1970, 1, 1)\n",
    "        return int((dt - epoch).total_seconds() * 1_000_000)\n",
    "\n",
    "    # Data structures to hold events\n",
    "    l0_stalls, memtable_stalls, pending_compaction_stalls = [], [], []\n",
    "    flush_data = {}\n",
    "    compaction_data = {}\n",
    "    memtable_stops = []\n",
    "\n",
    "    # Parse the LOG file\n",
    "    with open(log_file_path, 'r') as log_file:\n",
    "        for line in log_file:\n",
    "            # L0 stalls\n",
    "            if l0_stall_pattern.search(line):\n",
    "                ts_str = line.split(' ')[0]\n",
    "                timestamp_micros = timestamp_to_micros(ts_str)\n",
    "                rate = int(l0_stall_pattern.search(line).group(1)) / (1024*1024)\n",
    "                l0_stalls.append((timestamp_micros, rate))\n",
    "\n",
    "            # Memtable stalls\n",
    "            if memtable_stall_pattern.search(line):\n",
    "                ts_str = line.split(' ')[0]\n",
    "                timestamp_micros = timestamp_to_micros(ts_str)\n",
    "                rate = int(memtable_stall_pattern.search(line).group(1)) / (1024*1024)\n",
    "                memtable_stalls.append((timestamp_micros, rate))\n",
    "\n",
    "            # Pending compaction stalls\n",
    "            if pending_compaction_stall_pattern.search(line):\n",
    "                ts_str = line.split(' ')[0]\n",
    "                timestamp_micros = timestamp_to_micros(ts_str)\n",
    "                rate = int(pending_compaction_stall_pattern.search(line).group(1)) / (1024*1024)\n",
    "                pending_compaction_stalls.append((timestamp_micros, rate))\n",
    "\n",
    "            # Memtable stops\n",
    "            if memtable_stop_pattern.search(line):\n",
    "                ts_str = line.split(' ')[0]\n",
    "                timestamp_micros = timestamp_to_micros(ts_str)\n",
    "                memtable_stops.append((timestamp_micros, \"default\"))\n",
    "\n",
    "            # Flush events\n",
    "            flush_match = flush_regex.match(line)\n",
    "            if flush_match:\n",
    "                timestamp_str, cf_name, flush_microseconds, flush_bytes = flush_match.groups()\n",
    "                start_time_seconds = timestamp_to_seconds(timestamp_str) - int(flush_microseconds)/1e6\n",
    "                rate_MB_s = (int(flush_bytes) / int(flush_microseconds)) * 1e6 / (1024**2)\n",
    "                if \"flush\" not in flush_data:\n",
    "                    flush_data[\"flush\"] = []\n",
    "                flush_data[\"flush\"].append((start_time_seconds, rate_MB_s, int(flush_microseconds)/1e6))\n",
    "\n",
    "            # Compaction events\n",
    "            compaction_match = compaction_regex.match(line)\n",
    "            if compaction_match:\n",
    "                json_str = compaction_match.group(1)\n",
    "                try:\n",
    "                    event_data = json.loads(json_str)\n",
    "                    if event_data['event'] == 'compaction_finished':\n",
    "                        end_time_seconds = event_data['time_micros'] / 1e6\n",
    "                        start_time_seconds = end_time_seconds - event_data['compaction_time_micros'] / 1e6\n",
    "                        write_rate = event_data['write_rate']\n",
    "                        output_level = event_data['output_level']\n",
    "                        if \"compaction\" not in compaction_data:\n",
    "                            compaction_data[\"compaction\"] = []\n",
    "                        compaction_data[\"compaction\"].append((start_time_seconds, end_time_seconds, write_rate, output_level))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    # Adjust timestamps by experiment_start_time\n",
    "    l0_timestamps = [(ts/1e6) - experiment_start_time for ts,_ in l0_stalls]\n",
    "    l0_rates = [r for _,r in l0_stalls]\n",
    "\n",
    "    memtable_timestamps = [(ts/1e6)-experiment_start_time for ts,_ in memtable_stalls]\n",
    "    memtable_rates = [r for _,r in memtable_stalls]\n",
    "\n",
    "    pending_compaction_timestamps = [(ts/1e6)-experiment_start_time for ts,_ in pending_compaction_stalls]\n",
    "    pending_compaction_rates = [r for _,r in pending_compaction_stalls]\n",
    "\n",
    "    # Plot stalls\n",
    "    ax.scatter(l0_timestamps, l0_rates, label='L0 Stalls', color='blue', s=10)\n",
    "    ax.scatter(memtable_timestamps, memtable_rates, label='Memtable Stalls', color='purple', s=10)\n",
    "    ax.scatter(pending_compaction_timestamps, pending_compaction_rates, label='Pend Compact Stalls', color='orange', s=10)\n",
    "\n",
    "    # Set flush and compaction colors\n",
    "    flush_color = 'blue'  # Flushes are always blue\n",
    "    # Red palette for compactions (shades of red based on output level)\n",
    "    compaction_colors = [\"#ffc1c1\", \"#fc9598\", \"#ff696e\", \"#f02225\", \"#cf1d20\", \"#ab1619\", \"#851114\", \"#610c0e\"]\n",
    "\n",
    "    def get_compaction_color(level):\n",
    "        # Pick a color based on the output level\n",
    "        idx = min(level-1, len(compaction_colors)-1)\n",
    "        return compaction_colors[idx]\n",
    "\n",
    "    # Only two labels: \"Flush\" and \"Compaction\"\n",
    "    flush_label_added = False\n",
    "    compaction_label_added = False\n",
    "\n",
    "    # Plot Flushes\n",
    "    if \"flush\" in flush_data:\n",
    "        for i, (start_time, rate, duration) in enumerate(flush_data[\"flush\"]):\n",
    "            lbl = \"Flush\" if not flush_label_added else \"\"\n",
    "            flush_label_added = True\n",
    "            ax.hlines(rate,\n",
    "                      start_time - experiment_start_time,\n",
    "                      (start_time + duration) - experiment_start_time,\n",
    "                      colors=flush_color, linewidth=4, linestyles='solid', label=lbl)\n",
    "\n",
    "    # Plot Compactions\n",
    "    if \"compaction\" in compaction_data:\n",
    "        for i, (start_time, end_time, write_rate, output_level) in enumerate(compaction_data[\"compaction\"]):\n",
    "            c_color = get_compaction_color(output_level)\n",
    "            lbl = \"Compaction\" if not compaction_label_added else \"\"\n",
    "            compaction_label_added = True\n",
    "            ax.hlines(write_rate,\n",
    "                      start_time - experiment_start_time,\n",
    "                      end_time - experiment_start_time,\n",
    "                      colors=c_color, linewidth=2, label=lbl)\n",
    "\n",
    "    # Plot memtable stops\n",
    "    for i, (ts, cf_name) in enumerate(memtable_stops):\n",
    "        stop_ts = (ts/1e6)-experiment_start_time\n",
    "        ax.axvline(x=stop_ts, color='brown', linestyle='--', linewidth=0.5, alpha=0.5,\n",
    "                   label='Memtable Stops' if i == 0 else \"\")\n",
    "\n",
    "    ax.set_title('Database Operations Over Time')\n",
    "    ax.set_xlabel('Time (seconds since start of experiment)')\n",
    "    ax.set_ylabel('MB/s')\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Adjusting subplot order\n",
    "experiment_start_time = df['timestamp'].min() / 1000.0\n",
    "xlim = (0, df['relative_time_s'].max())\n",
    "\n",
    "fig, axes = plt.subplots(nrows=11, ncols=1, figsize=(15, 35), sharex=True)\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# 1. Per-Client READ Throughput\n",
    "ax1 = axes[0]\n",
    "for client_id, grp in df_read.groupby('client_id'):\n",
    "    ax1.plot(grp['relative_time_s'], grp['throughput_mb_s'], label=f'Client {client_id}')\n",
    "ax1.set_title('Per-Client READ Throughput')\n",
    "ax1.set_ylabel('Throughput (MB/s)')\n",
    "ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# 2. Per-Client WRITE Throughput (INSERT & INSERT_BATCH)\n",
    "ax2 = axes[1]\n",
    "for client_id, grp in df_insert.groupby('client_id'):\n",
    "    ax2.plot(grp['relative_time_s'], grp['throughput_mb_s'], label=f'Client {client_id} INSERT')\n",
    "for client_id, grp in df_insert_batch.groupby('client_id'):\n",
    "    ax2.plot(grp['relative_time_s'], grp['throughput_mb_s'], label=f'Client {client_id} INSERT_BATCH')\n",
    "ax2.set_title('Per-Client WRITE Throughput (INSERT & INSERT_BATCH)')\n",
    "ax2.set_ylabel('Throughput (MB/s)')\n",
    "ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# 3. Combined READ & WRITE Throughput\n",
    "ax3 = axes[2]\n",
    "combined_ops = df_throughput[df_throughput['op_type'].isin(['READ', 'INSERT', 'INSERT_BATCH'])]\n",
    "for (client_id, op_type), grp in combined_ops.groupby(['client_id', 'op_type']):\n",
    "    ax3.plot(grp['relative_time_s'], grp['throughput_mb_s'], label=f'Client {client_id}, {op_type}')\n",
    "ax3.set_title('Combined READ & WRITE Throughput')\n",
    "ax3.set_ylabel('Throughput (MB/s)')\n",
    "ax3.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# 4. P99 READ Latency\n",
    "ax4 = axes[3]\n",
    "for client_id, grp in df_read_latency.groupby('client_id'):\n",
    "    ax4.plot(grp['relative_time_s'], grp['99p'], label=f'Client {client_id}')\n",
    "ax4.set_title('Per-Client READ P99 Latency')\n",
    "ax4.set_ylabel('Latency (ms)')\n",
    "ax4.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# 5. P99 INSERT Latency\n",
    "ax5 = axes[4]\n",
    "for client_id, grp in df_insert_latency.groupby('client_id'):\n",
    "    ax5.plot(grp['relative_time_s'], grp['99p'], label=f'Client {client_id}')\n",
    "ax5.set_title('Per-Client INSERT P99 Latency')\n",
    "ax5.set_ylabel('Latency (ms)')\n",
    "ax5.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# 6. P99 QUEUE Latency\n",
    "ax6 = axes[5]\n",
    "for client_id, grp in df_queue_latency.groupby('client_id'):\n",
    "    ax6.plot(grp['relative_time_s'], grp['99p'], label=f'Client {client_id}')\n",
    "ax6.set_title('Per-Client QUEUE P99 Latency')\n",
    "ax6.set_ylabel('Latency (ms)')\n",
    "ax6.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# 7. Memtable Shares Over Time\n",
    "ax7 = axes[6]\n",
    "plot_memtable_shares(ax7, \"logs/memtable_stats.txt\")\n",
    "\n",
    "# 8. Database Operations Over Time\n",
    "ax8 = axes[7]\n",
    "plot_database_operations(ax8, experiment_start_time=experiment_start_time, xlim=xlim)\n",
    "\n",
    "# 9. User Cache Hit Rate (per-client line)\n",
    "ax9 = axes[8]\n",
    "for client_id, grp in df_read.groupby('client_id'):\n",
    "    valid_grp = grp.dropna(subset=['user_cache_hit_rate'])\n",
    "    if not valid_grp.empty:\n",
    "        ax9.plot(valid_grp['relative_time_s'], valid_grp['user_cache_hit_rate'], label=f'Client {client_id}')\n",
    "ax9.set_title('Per-Client User Cache Hit Rate')\n",
    "ax9.set_ylabel('Hit Rate (%)')\n",
    "ax9.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# 10. User Cache Usage (Stacked)\n",
    "ax10 = axes[9]\n",
    "x = df_cache_usage_pivot.index\n",
    "y = df_cache_usage_pivot.values.T\n",
    "ax10.stackplot(x, y, labels=df_cache_usage_pivot.columns)\n",
    "ax10.set_title('Per-Client User Cache Usage (Stacked)')\n",
    "ax10.set_ylabel('Cache Usage')\n",
    "ax10.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# 11. SSD Throughput Over Time\n",
    "df_iostat = pd.read_csv(\"iostat_results.csv\")\n",
    "time_seconds = np.arange(len(df_iostat))\n",
    "start_time_shift = 0\n",
    "time_seconds = [t + start_time_shift for t in time_seconds]\n",
    "\n",
    "ax11 = axes[10]\n",
    "ax11.plot(time_seconds, df_iostat[\"rMB/s\"], label='Read MB/s', marker='o', color='tab:green')\n",
    "ax11.plot(time_seconds, df_iostat[\"wMB/s\"], label='Write MB/s', marker='o', color='tab:red')\n",
    "ax11.set_title('SSD Throughput Over Time')\n",
    "ax11.set_xlabel('Time (s)')\n",
    "ax11.set_ylabel('MB/s')\n",
    "ax11.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax11.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
