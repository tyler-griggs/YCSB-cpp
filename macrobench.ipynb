{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def plot_wbm_data(file_path):\n",
    "    # Initialize a nested dictionary to store data for each client id\n",
    "    client_data = defaultdict(lambda: {'global': [], 'steady': []})\n",
    "\n",
    "    # Read the file and parse relevant lines\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"wbm\"):\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) == 7 and parts[3] == 'res':\n",
    "                    # Process 'res' line with steady or global\n",
    "                    try:\n",
    "                        _, timestamp, client_id, operation, operation_size, current_value, steady_or_global = parts\n",
    "                        timestamp = int(timestamp) / 1000  # Convert to seconds\n",
    "                        client_id = int(client_id)\n",
    "                        current_value = int(current_value) / (1024 * 1024)  # Convert to MB\n",
    "                        steady_or_global = steady_or_global.strip()\n",
    "                        if steady_or_global in ['steady', 'global']:\n",
    "                            client_data[client_id][steady_or_global].append((timestamp, current_value))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                elif len(parts) == 8 and parts[3] == 'free':\n",
    "                    # Process 'free' line with global:XXX and steady:YYY\n",
    "                    try:\n",
    "                        _, timestamp, client_id, operation, operation_size, current_value, global_str, steady_str = parts\n",
    "                        timestamp = int(timestamp) / 1000\n",
    "                        client_id = int(client_id)\n",
    "                        # Extract the global and steady values\n",
    "                        global_value = int(global_str.split(':')[1]) / (1024 * 1024)  # Convert to MB\n",
    "                        steady_value = int(steady_str.split(':')[1]) / (1024 * 1024)  # Convert to MB\n",
    "                        # Store the values\n",
    "                        client_data[client_id]['global'].append((timestamp, global_value))\n",
    "                        client_data[client_id]['steady'].append((timestamp, steady_value))\n",
    "                    except (ValueError, IndexError):\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # Determine the start time of the experiment\n",
    "    start_time = min(\n",
    "        min((timestamps[0][0] for timestamps in client.values() if len(timestamps) > 0), default=float('inf'))\n",
    "        for client in client_data.values()\n",
    "    )\n",
    "\n",
    "    # Adjust timestamps and convert to numpy arrays\n",
    "    for client_id in client_data:\n",
    "        for key in ['global', 'steady']:\n",
    "            values = client_data[client_id][key]\n",
    "            adjusted_values = [(timestamp - start_time, value) for timestamp, value in values]\n",
    "            client_data[client_id][key] = np.array(adjusted_values)\n",
    "\n",
    "    # Collect all timestamps\n",
    "    all_timestamps = sorted(set(\n",
    "        timestamp\n",
    "        for client_values in client_data.values()\n",
    "        for key in ['global', 'steady']\n",
    "        for timestamp, _ in client_values[key]\n",
    "    ))\n",
    "\n",
    "    # Initialize latest values for each client and key\n",
    "    latest_values = {client_id: {'global': 0, 'steady': 0} for client_id in client_data}\n",
    "    total_usage_values = []\n",
    "    sum_steady_values = []\n",
    "    sum_global_values = []\n",
    "\n",
    "    # Calculate per-client total usage and total usages\n",
    "    for ts in all_timestamps:\n",
    "        total_global = 0\n",
    "        total_steady = 0\n",
    "        total_usage = 0\n",
    "        per_client_total_usage = {}\n",
    "        for client_id in client_data:\n",
    "            total_client_usage = 0\n",
    "            for key in ['global', 'steady']:\n",
    "                values = client_data[client_id][key]\n",
    "                if len(values) == 0:\n",
    "                    continue\n",
    "                mask = values[:, 0] <= ts\n",
    "                if np.any(mask):\n",
    "                    latest_values[client_id][key] = values[mask, 1][-1]\n",
    "                # Sum up the latest values\n",
    "            # Sum per-client total usage\n",
    "            total_client_usage = latest_values[client_id]['global'] + latest_values[client_id]['steady']\n",
    "            per_client_total_usage[client_id] = total_client_usage\n",
    "            total_usage += total_client_usage\n",
    "            total_global += latest_values[client_id]['global']\n",
    "            total_steady += latest_values[client_id]['steady']\n",
    "        total_usage_values.append((ts, total_usage))\n",
    "        sum_steady_values.append((ts, total_steady))\n",
    "        sum_global_values.append((ts, total_global))\n",
    "\n",
    "    # Convert total usage values to numpy arrays\n",
    "    total_usage_values = np.array(total_usage_values)\n",
    "    sum_steady_values = np.array(sum_steady_values)\n",
    "    sum_global_values = np.array(sum_global_values)\n",
    "\n",
    "    # Prepare per-client total usage arrays for plotting\n",
    "    client_total_usage = {}\n",
    "    for client_id in client_data:\n",
    "        timestamps = []\n",
    "        usage_values = []\n",
    "        latest_values = {'global': 0, 'steady': 0}\n",
    "        for ts in all_timestamps:\n",
    "            for key in ['global', 'steady']:\n",
    "                values = client_data[client_id][key]\n",
    "                if len(values) == 0:\n",
    "                    continue\n",
    "                mask = values[:, 0] <= ts\n",
    "                if np.any(mask):\n",
    "                    latest_values[key] = values[mask, 1][-1]\n",
    "            total_client_usage = latest_values['global'] + latest_values['steady']\n",
    "            timestamps.append(ts)\n",
    "            usage_values.append(total_client_usage)\n",
    "        client_total_usage[client_id] = np.array([timestamps, usage_values])\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot per-client total usage\n",
    "    for client_id, data in client_total_usage.items():\n",
    "        plt.plot(data[0], data[1], label=f\"Client {client_id} Total Usage\")\n",
    "\n",
    "    # Plot summed lines\n",
    "    plt.plot(total_usage_values[:, 0], total_usage_values[:, 1], label=\"All Clients Total Usage\", color=\"black\", linestyle=\"-\", linewidth=2)\n",
    "    plt.plot(sum_steady_values[:, 0], sum_steady_values[:, 1], label=\"Total Steady Usage\", color=\"green\", linestyle=\"--\", linewidth=2)\n",
    "    plt.plot(sum_global_values[:, 0], sum_global_values[:, 1], label=\"Total Global Usage\", color=\"blue\", linestyle=\":\", linewidth=2)\n",
    "\n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel(\"Time Since Start (s)\")\n",
    "    plt.ylabel(\"Usage (MB)\")\n",
    "    plt.title(\"Usage Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    # plt.ylim(0, 256)\n",
    "    # plt.xlim(0, 25)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the path to your file\n",
    "file_path = \"logs/memtable_stats.txt\"  # Replace with your file's path\n",
    "plot_wbm_data(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration: Operation sizes (in KB)\n",
    "READ_SIZE_KB = 1  # Size of a single READ operation\n",
    "INSERT_SIZE_KB = 1  # Size of a single INSERT operation\n",
    "INSERT_BATCH_SIZE_KB = 100  # Size of a single INSERT_BATCH operation\n",
    "\n",
    "# Convert sizes to MB for throughput calculations\n",
    "READ_SIZE_MB = READ_SIZE_KB / 1024\n",
    "INSERT_SIZE_MB = INSERT_SIZE_KB / 1024\n",
    "INSERT_BATCH_SIZE_MB = INSERT_BATCH_SIZE_KB / 1024\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('logs/client_stats.log')\n",
    "\n",
    "# Convert timestamp to relative times (in seconds)\n",
    "df['relative_time_ms'] = df['timestamp'] - df['timestamp'].min()\n",
    "df['relative_time_s'] = df['relative_time_ms'] / 1000.0\n",
    "\n",
    "# Sort data and calculate time differences\n",
    "df = df.sort_values(by=['client_id', 'op_type', 'timestamp'])\n",
    "df['time_diff_s'] = df.groupby(['client_id', 'op_type'])['relative_time_s'].diff()\n",
    "\n",
    "# Calculate throughput (ops/sec) and convert to MB/s\n",
    "df['throughput'] = df['count'] / df['time_diff_s']\n",
    "df['throughput_mb_s'] = 0.0  # initialize as float\n",
    "\n",
    "df.loc[df['op_type'] == 'READ', 'throughput_mb_s'] = df['throughput'] * READ_SIZE_MB\n",
    "df.loc[df['op_type'] == 'INSERT', 'throughput_mb_s'] = df['throughput'] * INSERT_SIZE_MB\n",
    "df.loc[df['op_type'] == 'INSERT_BATCH', 'throughput_mb_s'] = df['throughput'] * INSERT_BATCH_SIZE_MB\n",
    "\n",
    "# Drop NaN throughput rows\n",
    "df_throughput = df.dropna(subset=['throughput_mb_s'])\n",
    "\n",
    "# Separate dataframes for different operation categories\n",
    "df_read = df_throughput[df_throughput['op_type'] == 'READ']\n",
    "df_insert = df_throughput[df_throughput['op_type'] == 'INSERT']\n",
    "df_insert_batch = df_throughput[df_throughput['op_type'] == 'INSERT_BATCH']\n",
    "df_queue_latency = df[df['op_type'] == 'QUEUE']\n",
    "df_read_latency = df[df['op_type'] == 'READ']\n",
    "df_insert_latency = df[df['op_type'] == 'INSERT']\n",
    "\n",
    "# Calculate user_cache_hit_rate for READ operations\n",
    "df_read['user_cache_total'] = df_read['user_cache_hits'] + df_read['user_cache_misses']\n",
    "df_read['user_cache_hit_rate'] = np.where(\n",
    "    df_read['user_cache_total'] > 0,\n",
    "    (df_read['user_cache_hits'] / df_read['user_cache_total']) * 100.0,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Prepare data for user_cache_usage stacked area plot\n",
    "df_cache_usage = df_read[['relative_time_s', 'client_id', 'user_cache_usage']].dropna()\n",
    "df_cache_usage_pivot = df_cache_usage.pivot_table(\n",
    "    index='relative_time_s',\n",
    "    columns='client_id',\n",
    "    values='user_cache_usage',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Create figure and axes\n",
    "fig, axes = plt.subplots(nrows=8, ncols=1, figsize=(10, 24), sharex=True)\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# 1. Per-Client READ Throughput\n",
    "ax1 = axes[0]\n",
    "for client_id, grp in df_read.groupby('client_id'):\n",
    "    ax1.plot(grp['relative_time_s'], grp['throughput_mb_s'], label=f'Client {client_id}')\n",
    "ax1.set_title('Per-Client READ Throughput')\n",
    "ax1.set_ylabel('Throughput (MB/s)')\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "# 2. Per-Client WRITE Throughput (INSERT & INSERT_BATCH)\n",
    "ax2 = axes[1]\n",
    "for client_id, grp in df_insert.groupby('client_id'):\n",
    "    ax2.plot(grp['relative_time_s'], grp['throughput_mb_s'], label=f'Client {client_id} INSERT')\n",
    "for client_id, grp in df_insert_batch.groupby('client_id'):\n",
    "    ax2.plot(grp['relative_time_s'], grp['throughput_mb_s'], label=f'Client {client_id} INSERT_BATCH')\n",
    "ax2.set_title('Per-Client WRITE Throughput (INSERT & INSERT_BATCH)')\n",
    "ax2.set_ylabel('Throughput (MB/s)')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# 3. Combined READ & WRITE Throughput (in one plot, line per-client per-op)\n",
    "ax3 = axes[2]\n",
    "combined_ops = df_throughput[df_throughput['op_type'].isin(['READ', 'INSERT', 'INSERT_BATCH'])]\n",
    "for (client_id, op_type), grp in combined_ops.groupby(['client_id', 'op_type']):\n",
    "    ax3.plot(grp['relative_time_s'], grp['throughput_mb_s'], label=f'Client {client_id}, {op_type}')\n",
    "ax3.set_title('Combined READ & WRITE Throughput')\n",
    "ax3.set_ylabel('Throughput (MB/s)')\n",
    "ax3.legend(loc='upper right')\n",
    "\n",
    "# 4. P99 READ Latency\n",
    "ax4 = axes[3]\n",
    "for client_id, grp in df_read_latency.groupby('client_id'):\n",
    "    ax4.plot(grp['relative_time_s'], grp['99p'], label=f'Client {client_id}')\n",
    "ax4.set_title('Per-Client READ P99 Latency')\n",
    "ax4.set_ylabel('Latency (ms)')\n",
    "ax4.legend(loc='upper right')\n",
    "\n",
    "# 5. P99 INSERT Latency\n",
    "ax5 = axes[4]\n",
    "for client_id, grp in df_insert_latency.groupby('client_id'):\n",
    "    ax5.plot(grp['relative_time_s'], grp['99p'], label=f'Client {client_id}')\n",
    "ax5.set_title('Per-Client INSERT P99 Latency')\n",
    "ax5.set_ylabel('Latency (ms)')\n",
    "ax5.legend(loc='upper right')\n",
    "\n",
    "# 6. P99 QUEUE Latency\n",
    "ax6 = axes[5]\n",
    "for client_id, grp in df_queue_latency.groupby('client_id'):\n",
    "    ax6.plot(grp['relative_time_s'], grp['99p'], label=f'Client {client_id}')\n",
    "ax6.set_title('Per-Client QUEUE P99 Latency')\n",
    "ax6.set_ylabel('Latency (ms)')\n",
    "ax6.legend(loc='upper right')\n",
    "\n",
    "# # 7. User Cache Hit Rate (per-client line)\n",
    "# ax7 = axes[6]\n",
    "# for client_id, grp in df_read.groupby('client_id'):\n",
    "#     valid_grp = grp.dropna(subset=['user_cache_hit_rate'])\n",
    "#     if not valid_grp.empty:\n",
    "#         ax7.plot(valid_grp['relative_time_s'], valid_grp['user_cache_hit_rate'], label=f'Client {client_id}')\n",
    "# ax7.set_title('Per-Client User Cache Hit Rate')\n",
    "# ax7.set_ylabel('Hit Rate (%)')\n",
    "# ax7.legend(loc='upper right')\n",
    "\n",
    "# # 8. User Cache Usage (Stacked)\n",
    "# ax8 = axes[7]\n",
    "# x = df_cache_usage_pivot.index\n",
    "# y = df_cache_usage_pivot.values.T  # Each row in y is a series for one client\n",
    "# ax8.stackplot(x, y, labels=df_cache_usage_pivot.columns)\n",
    "# ax8.set_title('Per-Client User Cache Usage (Stacked)')\n",
    "# ax8.set_xlabel('Time (s)')\n",
    "# ax8.set_ylabel('Cache Usage')\n",
    "# ax8.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall throughputs (client + system)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "max_read_tput = 700\n",
    "max_read_iops = 180000\n",
    "max_write_tput = 400\n",
    "max_write_iops = 100000\n",
    "\n",
    "def timestamp_to_seconds(timestamp_str):\n",
    "  timestamp = datetime.strptime(timestamp_str.rstrip(), '%Y-%m-%d %H:%M:%S.%f')\n",
    "  epoch = datetime(1970, 1, 1)\n",
    "  return (timestamp - epoch).total_seconds()\n",
    "\n",
    "def plot_overall_tputs(output_file, axs, start_time_shift, xlim, fig_loc):\n",
    "  df_from_csv = pd.read_csv(\"iostat_results.csv\")\n",
    "\n",
    "  # print(f\"Timestamp: {timestamp_to_seconds(df_from_csv['Timestamp'][0])}\")\n",
    "\n",
    "  time_seconds = np.arange(len(df_from_csv))\n",
    "  time_seconds = [x + start_time_shift for x in time_seconds]\n",
    "\n",
    "  # Plotting\n",
    "  # axs[fig_loc[0]].figure(figsize=(10, 6))\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"rMB/s\"], label='Read MB/s', marker='o', color='tab:green')\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"wMB/s\"], label='Write MB/s', marker='o', color='tab:red')\n",
    "\n",
    "  with open(output_file, 'a') as outfile:\n",
    "    outfile.write(f\"metric-rMB_rate\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"rMB/s\"].tolist()}\\n\")\n",
    "\n",
    "    outfile.write(f\"metric-wMB_rate\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"wMB/s\"].tolist()}\\n\")\n",
    "\n",
    "  axs[fig_loc[0]].set_title('SSD Throughput Over Time')\n",
    "  axs[fig_loc[0]].set_xlabel('Time (s)')\n",
    "  axs[fig_loc[0]].set_ylabel('MB/s')\n",
    "  axs[fig_loc[0]].set_xlim(xlim)\n",
    "  # axs[fig_loc[0]].set_ylim(0, 520)\n",
    "  axs[fig_loc[0]].legend(loc='upper left')\n",
    "  axs[fig_loc[0]].grid(True)\n",
    "\n",
    "  # # Creating a second y-axis\n",
    "  # ax2 = axs[fig_loc[0]].twinx()\n",
    "  # # Plotting on the secondary y-axis\n",
    "  # ax2.plot(time_seconds, df_from_csv[\"rMB/s\"]/max_read_tput, label='Read Util', marker='x', linestyle='--', color='tab:green')\n",
    "  # ax2.plot(time_seconds, df_from_csv[\"wMB/s\"]/max_write_tput, label='Write Util', marker='+', linestyle='--', color='tab:red')\n",
    "  # ax2.set_ylabel('Utilization (based on tput)')\n",
    "  # ax2.legend(loc='upper right')\n",
    "  # ax2.set_ylim(0,1)\n",
    "\n",
    "  # Adjust the right margin to accommodate the second y-axis legend\n",
    "  plt.subplots_adjust(right=0.85)\n",
    "\n",
    "  with open(output_file, 'a') as outfile:\n",
    "    outfile.write(f\"metric-rMB_util\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{(df_from_csv[\"rMB/s\"]/max_read_tput).tolist()}\\n\")\n",
    "\n",
    "    outfile.write(f\"metric-wMB_util\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{(df_from_csv[\"wMB/s\"]/max_write_tput).tolist()}\\n\")\n",
    "\n",
    "def plot_overall_iops(output_file, axs, start_time_shift, xlim, fig_loc):\n",
    "  df_from_csv = pd.read_csv(\"iostat_results.csv\")\n",
    "  time_seconds = np.arange(len(df_from_csv))\n",
    "  time_seconds = [x + start_time_shift for x in time_seconds]\n",
    "\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"r/s\"], label='Read IOPS', marker='o', color='tab:green')\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"w/s\"], label='Write IOPS ', marker='o', color='tab:red')\n",
    "\n",
    "  with open(output_file, 'a') as outfile:\n",
    "    outfile.write(f\"metric-rIOP_rate\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"r/s\"].tolist()}\\n\")\n",
    "\n",
    "    outfile.write(f\"metric-wIOP_rate\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"w/s\"].tolist()}\\n\")\n",
    "\n",
    "  axs[fig_loc[0]].set_title('SSD IOPS Over Time')\n",
    "  axs[fig_loc[0]].set_xlabel('Time (s)')\n",
    "  axs[fig_loc[0]].set_ylabel('IOPS')\n",
    "  axs[fig_loc[0]].set_xlim(xlim)\n",
    "  # axs[fig_loc[0]].set_ylim(0,16)\n",
    "  axs[fig_loc[0]].legend(loc='upper right')\n",
    "  axs[fig_loc[0]].grid(True)\n",
    "\n",
    "  # Creating a second y-axis\n",
    "  ax2 = axs[fig_loc[0]].twinx()\n",
    "  # Plotting on the secondary y-axis\n",
    "  ax2.plot(time_seconds, df_from_csv[\"r/s\"]/max_read_iops, label='Read IOPS Util', marker='x', linestyle='--', color='tab:green')\n",
    "  ax2.plot(time_seconds, df_from_csv[\"w/s\"]/max_write_iops, label='Write IOPS Util', marker='+', linestyle='--', color='tab:red')\n",
    "  ax2.set_ylabel('Utilization (based on iops)')\n",
    "  ax2.legend(loc='upper right')\n",
    "  ax2.set_ylim(0,1)\n",
    "\n",
    "  # Adjust the right margin to accommodate the second y-axis legend\n",
    "  plt.subplots_adjust(right=0.85)\n",
    "\n",
    "  with open(output_file, 'a') as outfile:\n",
    "    outfile.write(f\"metric-rIOP_util\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{(df_from_csv[\"r/s\"]/max_read_iops).tolist()}\\n\")\n",
    "\n",
    "    outfile.write(f\"metric-wIOP_util\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{(df_from_csv[\"w/s\"]/max_write_iops).tolist()}\\n\")\n",
    "\n",
    "def plot_io_waittimes(output_file, axs, start_time_shift, xlim, fig_loc):\n",
    "  df_from_csv = pd.read_csv(\"iostat_results.csv\")\n",
    "  time_seconds = np.arange(len(df_from_csv))\n",
    "  time_seconds = [x + start_time_shift for x in time_seconds]\n",
    "\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"r_await\"], label='Read Await (per req)', marker='o', color='tab:green')\n",
    "  # axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"w_await\"], label='Write Await (per req)', marker='o', color='tab:red')\n",
    "\n",
    "  axs[fig_loc[0]].set_title('IO Wait Times (queueing + servicing)')\n",
    "  axs[fig_loc[0]].set_xlabel('Time (s)')\n",
    "  axs[fig_loc[0]].set_ylabel('Wait Time (ms)')\n",
    "  axs[fig_loc[0]].set_xlim(xlim)\n",
    "  # axs[fig_loc[0]].set_ylim(0,1)\n",
    "  axs[fig_loc[0]].legend(loc='upper left')\n",
    "  axs[fig_loc[0]].grid(True)\n",
    "\n",
    "  ax2 = axs[fig_loc[0]].twinx()\n",
    "  ax2.plot(time_seconds, [df_from_csv[\"r_await\"][i] / df_from_csv[\"rareq-sz\"][i] if df_from_csv[\"rareq-sz\"][i] > 0 else df_from_csv[\"r_await\"][i] for i in range(len(df_from_csv[\"r_await\"]))], label='Read Await (per KB)', marker='x', color='tab:green')\n",
    "  # ax2.plot(time_seconds, [df_from_csv[\"w_await\"][i] / df_from_csv[\"wareq-sz\"][i] if df_from_csv[\"wareq-sz\"][i] > 0 else df_from_csv[\"w_await\"][i]   for i in range(len(df_from_csv[\"w_await\"]))], label='Write Await (per KB)', marker='x', color='tab:red')\n",
    "\n",
    "  ax2.set_ylabel('IO Wait Times per KB')\n",
    "  ax2.legend(loc='upper right')\n",
    "  plt.subplots_adjust(right=0.85)\n",
    "\n",
    "  with open(output_file, 'a') as outfile:\n",
    "    outfile.write(f\"metric-r_await\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"r_await\"].tolist()}\\n\")\n",
    "\n",
    "    outfile.write(f\"metric-r_await_per_kb\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{[df_from_csv[\"r_await\"][i] / df_from_csv[\"rareq-sz\"][i] if df_from_csv[\"rareq-sz\"][i] > 0 else df_from_csv[\"r_await\"][i] for i in range(len(df_from_csv[\"r_await\"]))]}\\n\")\n",
    "\n",
    "def plot_io_reqsize(output_file, axs, start_time_shift, xlim, fig_loc):\n",
    "  df_from_csv = pd.read_csv(\"iostat_results.csv\")\n",
    "  time_seconds = np.arange(len(df_from_csv))\n",
    "  time_seconds = [x + start_time_shift for x in time_seconds]\n",
    "\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"rareq-sz\"], label='Avg Read Size', marker='o', color='tab:green')\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"wareq-sz\"], label='Avg Write Size', marker='o', color='tab:red')\n",
    "\n",
    "  axs[fig_loc[0]].set_title('Avg IO Sizes')\n",
    "  axs[fig_loc[0]].set_xlabel('Time (s)')\n",
    "  axs[fig_loc[0]].set_ylabel('Size (KB)')\n",
    "  axs[fig_loc[0]].set_xlim(xlim)\n",
    "  # axs[fig_loc[0]].set_ylim(0,1)\n",
    "  axs[fig_loc[0]].legend(loc='upper right')\n",
    "  axs[fig_loc[0]].grid(True)\n",
    "\n",
    "  with open(output_file, 'a') as outfile:\n",
    "    outfile.write(f\"metric-r_size\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"rareq-sz\"].tolist()}\\n\")\n",
    "\n",
    "    outfile.write(f\"metric-w_size\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"wareq-sz\"].tolist()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
