{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib \n",
    "!pip install pandas\n",
    "!pip install plotly\n",
    "!pip install nbformat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Constants\n",
    "READ_SIZE_MB = 64/1024\n",
    "INSERT_SIZE_MB = 1/1024\n",
    "INSERT_BATCH_SIZE_MB = 100/1024\n",
    "\n",
    "# Load + preprocess main log\n",
    "df = pd.read_csv('logs/client_stats.log')\n",
    "df['relative_time_s'] = (df['timestamp'] - df['timestamp'].min()) / 1000\n",
    "df = df.sort_values(['client_id','op_type','timestamp'])\n",
    "df['time_diff_s'] = df.groupby(['client_id','op_type'])['relative_time_s'].diff()\n",
    "df['throughput'] = df['count']/df['time_diff_s']\n",
    "df['throughput_mb_s'] = np.select(\n",
    "    [df.op_type=='READ', df.op_type=='INSERT', df.op_type=='INSERT_BATCH'],\n",
    "    [df.throughput*READ_SIZE_MB, df.throughput*INSERT_SIZE_MB, df.throughput*INSERT_BATCH_SIZE_MB],\n",
    "    default=np.nan\n",
    ")\n",
    "df_th = df.dropna(subset=['throughput_mb_s'])\n",
    "\n",
    "# Split by op_type\n",
    "df_read = df_th[df_th.op_type=='READ'].copy()\n",
    "df_insert = df_th[df_th.op_type=='INSERT']\n",
    "df_insert_batch = df_th[df_th.op_type=='INSERT_BATCH']\n",
    "df_queue = df[df.op_type=='QUEUE']\n",
    "df_read_latency = df[df.op_type=='READ']\n",
    "df_insert_latency = df[df.op_type=='INSERT']\n",
    "\n",
    "# Cache metrics\n",
    "df_read['user_cache_total'] = df_read.user_cache_hits + df_read.user_cache_misses\n",
    "df_read['user_cache_hit_rate'] = np.where(df_read.user_cache_total>0,\n",
    "                                          df_read.user_cache_hits/df_read.user_cache_total*100,\n",
    "                                          np.nan)\n",
    "df_cache = df_read[['relative_time_s','client_id','user_cache_usage']].dropna()\n",
    "pivot_cache = df_cache.pivot_table(index='relative_time_s',\n",
    "                                   columns='client_id',\n",
    "                                   values='user_cache_usage',\n",
    "                                   aggfunc='mean').fillna(0)\n",
    "\n",
    "# Create subplots (13 rows)\n",
    "fig = make_subplots(rows=13, cols=1, shared_xaxes=True, vertical_spacing=0.01,\n",
    "                    subplot_titles=[\n",
    "                        f\"Per-Client READ Throughput for read size {READ_SIZE_MB*1024:.0f}KB\",\n",
    "                        \"Per-Client WRITE Throughput (INSERT & INSERT_BATCH)\",\n",
    "                        \"Combined READ & WRITE Throughput\",\n",
    "                        \"Per-Client READ P10 Latency\", \"Per-Client READ P25 Latency\",\n",
    "                        \"Per-Client READ P50 Latency\", \"Per-Client READ P99 Latency\",\n",
    "                        \"Per-Client READ Max Latency\", \"Per-Client INSERT P99 Latency\",\n",
    "                        \"Per-Client QUEUE P99 Latency\", \"Per-Client User Cache Hit Rate\",\n",
    "                        \"Per-Client User Cache Usage (Stacked)\",\n",
    "                        \"SSD Throughput Over Time\"\n",
    "                    ])\n",
    "\n",
    "def add_group(df_group, row, ycol, name_fmt):\n",
    "    for cid, grp in df_group.groupby('client_id'):\n",
    "        fig.add_trace(go.Scatter(x=grp.relative_time_s, y=grp[ycol], name=name_fmt.format(cid)),\n",
    "                      row=row, col=1)\n",
    "\n",
    "# Populate subplots 1–12\n",
    "add_group(df_read, 1, 'throughput_mb_s', \"Client {}\")\n",
    "add_group(df_insert, 2, 'throughput_mb_s', \"Client {} INSERT\")\n",
    "add_group(df_insert_batch, 2, 'throughput_mb_s', \"Client {} INSERT_BATCH\")\n",
    "for (cid, op), grp in df_th.groupby(['client_id','op_type']):\n",
    "    fig.add_trace(go.Scatter(x=grp.relative_time_s, y=grp.throughput_mb_s,\n",
    "                             name=f\"Client {cid}, {op}\"), row=3, col=1)\n",
    "for col_name, row_num in [('10p',4), ('25p',5), ('50p',6), ('99p',7), ('max',8)]:\n",
    "    add_group(df_read_latency, row_num, col_name, \"Client {}\")\n",
    "add_group(df_insert_latency, 9, '99p', \"Client {}\")\n",
    "add_group(df_queue, 10, '99p', \"Client {}\")\n",
    "add_group(df_read, 11, 'user_cache_hit_rate', \"Client {}\")\n",
    "for cid in pivot_cache.columns:\n",
    "    fig.add_trace(go.Scatter(x=pivot_cache.index, y=pivot_cache[cid]/1024**2,\n",
    "                             name=f\"Client {cid}\", stackgroup=\"one\"), row=12, col=1)\n",
    "\n",
    "# === New SSD throughput subplot (row 13) — first 1/5th only ===\n",
    "df_iostat = pd.read_csv(\"iostat_results.csv\")\n",
    "cutoff = len(df_iostat) // 5\n",
    "time_seconds = np.arange(cutoff) + df['relative_time_s'].min()\n",
    "fig.add_trace(go.Scatter(x=time_seconds, y=df_iostat[\"rMB/s\"].iloc[:cutoff], name=\"SSD Read MB/s\", mode=\"lines+markers\"), row=13, col=1)\n",
    "fig.add_trace(go.Scatter(x=time_seconds, y=df_iostat[\"wMB/s\"].iloc[:cutoff], name=\"SSD Write MB/s\", mode=\"lines+markers\"), row=13, col=1)\n",
    "\n",
    "# Axis labels\n",
    "y_labels = ['Throughput (MB/s)']*3 + ['Latency (ms)']*7 + ['Hit Rate (%)','Cache Usage (MB)','MB/s']\n",
    "for r,label in enumerate(y_labels, start=1):\n",
    "    fig.update_yaxes(title_text=label, row=r, col=1)\n",
    "fig.update_xaxes(title_text=\"Time (s)\", row=13, col=1)\n",
    "fig.update_yaxes(range=[0,400], row=5, col=1)\n",
    "\n",
    "fig.update_layout(height=3900, width=1000, showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "import json\n",
    "import colorsys\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "\n",
    "# ---------------------------\n",
    "# Main Log Processing & Plotting\n",
    "# ---------------------------\n",
    "\n",
    "# Constants\n",
    "READ_SIZE_MB = 64/1024\n",
    "INSERT_SIZE_MB = 1/1024\n",
    "INSERT_BATCH_SIZE_MB = 100/1024\n",
    "\n",
    "# Load + preprocess main log\n",
    "df = pd.read_csv('logs/client_stats.log')\n",
    "df['relative_time_s'] = (df['timestamp'] - df['timestamp'].min()) / 1000\n",
    "df = df.sort_values(['client_id','op_type','timestamp'])\n",
    "df['time_diff_s'] = df.groupby(['client_id','op_type'])['relative_time_s'].diff()\n",
    "df['throughput'] = df['count'] / df['time_diff_s']\n",
    "df['throughput_mb_s'] = np.select(\n",
    "    [df.op_type=='READ', df.op_type=='INSERT', df.op_type=='INSERT_BATCH'],\n",
    "    [df.throughput*READ_SIZE_MB, df.throughput*INSERT_SIZE_MB, df.throughput*INSERT_BATCH_SIZE_MB],\n",
    "    default=np.nan\n",
    ")\n",
    "df_th = df.dropna(subset=['throughput_mb_s'])\n",
    "\n",
    "# Split by op_type\n",
    "df_read = df_th[df_th.op_type=='READ'].copy()\n",
    "df_insert = df_th[df_th.op_type=='INSERT']\n",
    "df_insert_batch = df_th[df_th.op_type=='INSERT_BATCH']\n",
    "df_queue = df[df.op_type=='QUEUE']\n",
    "df_read_latency = df[df.op_type=='READ']\n",
    "df_insert_latency = df[df.op_type=='INSERT']\n",
    "\n",
    "# Cache metrics\n",
    "df_read['user_cache_total'] = df_read.user_cache_hits + df_read.user_cache_misses\n",
    "df_read['user_cache_hit_rate'] = np.where(df_read.user_cache_total>0,\n",
    "                                          df_read.user_cache_hits/df_read.user_cache_total*100,\n",
    "                                          np.nan)\n",
    "df_cache = df_read[['relative_time_s','client_id','user_cache_usage']].dropna()\n",
    "pivot_cache = df_cache.pivot_table(index='relative_time_s',\n",
    "                                   columns='client_id',\n",
    "                                   values='user_cache_usage',\n",
    "                                   aggfunc='mean').fillna(0)\n",
    "\n",
    "# Create subplots (14 rows now; row 14 is for RocksDB events)\n",
    "fig = make_subplots(rows=14, cols=1, shared_xaxes=True, vertical_spacing=0.01,\n",
    "                    subplot_titles=[\n",
    "                        f\"Per-Client READ Throughput for read size {READ_SIZE_MB*1024:.0f}KB\",\n",
    "                        \"Per-Client WRITE Throughput (INSERT & INSERT_BATCH)\",\n",
    "                        \"Combined READ & WRITE Throughput\",\n",
    "                        \"Per-Client READ P10 Latency\", \"Per-Client READ P25 Latency\",\n",
    "                        \"Per-Client READ P50 Latency\", \"Per-Client READ P99 Latency\",\n",
    "                        \"Per-Client READ Max Latency\", \"Per-Client INSERT P99 Latency\",\n",
    "                        \"Per-Client QUEUE P99 Latency\", \"Per-Client User Cache Hit Rate\",\n",
    "                        \"Per-Client User Cache Usage (Stacked)\",\n",
    "                        \"SSD Throughput Over Time\",\n",
    "                        \"RocksDB Events Over Time\"  # New subplot row 14\n",
    "                    ])\n",
    "\n",
    "def add_group(df_group, row, ycol, name_fmt):\n",
    "    for cid, grp in df_group.groupby('client_id'):\n",
    "        fig.add_trace(go.Scatter(x=grp.relative_time_s, y=grp[ycol], name=name_fmt.format(cid)),\n",
    "                      row=row, col=1)\n",
    "\n",
    "# Populate subplots 1–12\n",
    "add_group(df_read, 1, 'throughput_mb_s', \"Client {}\")\n",
    "add_group(df_insert, 2, 'throughput_mb_s', \"Client {} INSERT\")\n",
    "add_group(df_insert_batch, 2, 'throughput_mb_s', \"Client {} INSERT_BATCH\")\n",
    "for (cid, op), grp in df_th.groupby(['client_id','op_type']):\n",
    "    fig.add_trace(go.Scatter(x=grp.relative_time_s, y=grp.throughput_mb_s,\n",
    "                             name=f\"Client {cid}, {op}\"), row=3, col=1)\n",
    "for col_name, row_num in [('10p',4), ('25p',5), ('50p',6), ('99p',7), ('max',8)]:\n",
    "    add_group(df_read_latency, row_num, col_name, \"Client {}\")\n",
    "add_group(df_insert_latency, 9, '99p', \"Client {}\")\n",
    "add_group(df_queue, 10, '99p', \"Client {}\")\n",
    "add_group(df_read, 11, 'user_cache_hit_rate', \"Client {}\")\n",
    "for cid in pivot_cache.columns:\n",
    "    fig.add_trace(go.Scatter(x=pivot_cache.index, y=pivot_cache[cid]/1024**2,\n",
    "                             name=f\"Client {cid}\", stackgroup=\"one\"), row=12, col=1)\n",
    "\n",
    "# SSD Throughput subplot (row 13) — first 1/5th only\n",
    "df_iostat = pd.read_csv(\"iostat_results.csv\")\n",
    "# cutoff = len(df_iostat) // 5\n",
    "cutoff = len(df_iostat)\n",
    "time_seconds = np.arange(cutoff) + df['relative_time_s'].min()\n",
    "fig.add_trace(go.Scatter(x=time_seconds, y=df_iostat[\"rMB/s\"].iloc[:cutoff],\n",
    "                         name=\"SSD Read MB/s\", mode=\"lines+markers\"), row=13, col=1)\n",
    "fig.add_trace(go.Scatter(x=time_seconds, y=df_iostat[\"wMB/s\"].iloc[:cutoff],\n",
    "                         name=\"SSD Write MB/s\", mode=\"lines+markers\"), row=13, col=1)\n",
    "\n",
    "# Axis labels\n",
    "y_labels = ['Throughput (MB/s)']*3 + ['Latency (ms)']*7 + ['Hit Rate (%)','Cache Usage (MB)','MB/s', '']\n",
    "for r, label in enumerate(y_labels, start=1):\n",
    "    fig.update_yaxes(title_text=label, row=r, col=1)\n",
    "fig.update_xaxes(title_text=\"Time (s)\", row=14, col=1)\n",
    "fig.update_yaxes(range=[0,400], row=5, col=1)\n",
    "\n",
    "# ---------------------------\n",
    "# RocksDB Events Plot (Row 14)\n",
    "# ---------------------------\n",
    "# For this plot, we assume the experiment start time is 0 (since main log times are already relative)\n",
    "experiment_start_time = 0\n",
    "\n",
    "# --- Helper functions for scalable client coloring ---\n",
    "\n",
    "base_colors = {}\n",
    "\n",
    "def get_client_base_color(client):\n",
    "    \"\"\"\n",
    "    Returns a base color for the client. If the client does not have an assigned color,\n",
    "    assign one from Plotly's 'Dark24' qualitative palette.\n",
    "    \"\"\"\n",
    "    if client not in base_colors:\n",
    "        palette = px.colors.qualitative.Dark24\n",
    "        index = len(base_colors) % len(palette)\n",
    "        base_colors[client] = palette[index]\n",
    "    return base_colors[client]\n",
    "\n",
    "def shade_color(base_color, level):\n",
    "    \"\"\"\n",
    "    Returns a shade of the base color based on the level.\n",
    "    Levels range from 1 (lightest) to 7 (darkest). Adjusts the lightness.\n",
    "    \"\"\"\n",
    "    r, g, b = [int(base_color.lstrip('#')[i:i+2], 16)/255.0 for i in (0, 2, 4)]\n",
    "    h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "    new_l = 0.9 - (level - 1) * ((0.9 - 0.5) / 6)\n",
    "    new_r, new_g, new_b = colorsys.hls_to_rgb(h, new_l, s)\n",
    "    return '#%02x%02x%02x' % (int(new_r*255), int(new_g*255), int(new_b*255))\n",
    "\n",
    "def get_client_color(client, level):\n",
    "    \"\"\"\n",
    "    Returns a color for a given client and level by adjusting the client’s base color.\n",
    "    \"\"\"\n",
    "    base = get_client_base_color(client)\n",
    "    return shade_color(base, level)\n",
    "\n",
    "# --- Parsing RocksDB Log and Adding Traces ---\n",
    "# The log file path (adjust if needed)\n",
    "log_file_path = '/mnt/rocksdb/ycsb-rocksdb-data/LOG'\n",
    "\n",
    "# Compile regex patterns\n",
    "flush_regex = re.compile(\n",
    "    r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\d+ \\[/flush_job\\.cc:\\d+\\] \\[(.*?)\\] \\[JOB \\d+\\] Flush: (\\d+) microseconds, \\d+ cpu microseconds, (\\d+) bytes'\n",
    ")\n",
    "l0_stall_pattern = re.compile(\n",
    "    r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\d+ \\[WARN\\] \\[/column_family.cc:\\d+\\] \\[([^,]+)\\] Stalling writes because we have \\d+ level-0 files rate (\\d+)'\n",
    ")\n",
    "memtable_stall_pattern = re.compile(\n",
    "    r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\d+ \\[WARN\\] \\[/column_family.cc:\\d+\\] \\[([^,]+)\\] Stalling writes because we have \\d+ immutable memtables.*rate (\\d+)'\n",
    ")\n",
    "pending_compaction_stall_pattern = re.compile(\n",
    "    r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\d+ \\[WARN\\] \\[/column_family.cc:\\d+\\] \\[([^,]+)\\] Stalling writes because of estimated pending compaction bytes \\d+ rate (\\d+)'\n",
    ")\n",
    "memtable_stop_pattern = re.compile(\n",
    "    r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\d+ \\[WARN\\] \\[/column_family.cc:\\d+\\] \\[([^,]+)\\] Stopping writes because we have \\d+ immutable memtables.*'\n",
    ")\n",
    "compaction_regex = re.compile(r'.*EVENT_LOG_v1 (.*)$')\n",
    "\n",
    "def timestamp_to_seconds(timestamp_str):\n",
    "    ts = datetime.strptime(timestamp_str, '%Y/%m/%d-%H:%M:%S.%f')\n",
    "    epoch = datetime(1970, 1, 1)\n",
    "    return (ts - epoch).total_seconds()\n",
    "\n",
    "def timestamp_to_micros(timestamp_str):\n",
    "    dt = datetime.strptime(timestamp_str, '%Y/%m/%d-%H:%M:%S.%f')\n",
    "    epoch = datetime(1970, 1, 1)\n",
    "    return int((dt - epoch).total_seconds() * 1e6)\n",
    "\n",
    "# Containers for events\n",
    "l0_stalls = []\n",
    "memtable_stalls = []\n",
    "pending_compaction_stalls = []\n",
    "compaction_data = {}  # keyed by client (cf_name)\n",
    "flush_data = {}       # keyed by client\n",
    "memtable_stops = []\n",
    "\n",
    "with open(log_file_path, 'r') as log_file:\n",
    "    for line in log_file:\n",
    "        # L0 Stalls\n",
    "        m = l0_stall_pattern.search(line)\n",
    "        if m:\n",
    "            timestamp_str, cf_name, rate = m.groups()\n",
    "            ts = timestamp_to_micros(timestamp_str)\n",
    "            l0_stalls.append((ts, int(rate)/1024/1024))\n",
    "        # Memtable Stalls\n",
    "        m = memtable_stall_pattern.search(line)\n",
    "        if m:\n",
    "            timestamp_str, cf_name, rate = m.groups()\n",
    "            ts = timestamp_to_micros(timestamp_str)\n",
    "            memtable_stalls.append((ts, int(rate)/1024/1024))\n",
    "        # Memtable Stops\n",
    "        m = memtable_stop_pattern.search(line)\n",
    "        if m:\n",
    "            timestamp_str, cf_name = m.groups()\n",
    "            ts = timestamp_to_micros(timestamp_str)\n",
    "            memtable_stops.append((ts, cf_name))\n",
    "        # Pending Compaction Stalls\n",
    "        m = pending_compaction_stall_pattern.search(line)\n",
    "        if m:\n",
    "            timestamp_str, cf_name, rate = m.groups()\n",
    "            ts = timestamp_to_micros(timestamp_str)\n",
    "            pending_compaction_stalls.append((ts, int(rate)/1024/1024))\n",
    "        # Flush Events\n",
    "        m = flush_regex.match(line)\n",
    "        if m:\n",
    "            timestamp_str, cf_name, flush_micro, flush_bytes = m.groups()\n",
    "            start_sec = timestamp_to_seconds(timestamp_str) - int(flush_micro)/1e6\n",
    "            rate = (int(flush_bytes) / int(flush_micro)) * 1e6 / (1024**2)\n",
    "            flush_data.setdefault(cf_name, []).append((start_sec, rate, int(flush_micro)/1e6))\n",
    "        # Compaction Events\n",
    "        m = compaction_regex.match(line)\n",
    "        if m:\n",
    "            json_str = m.group(1)\n",
    "            try:\n",
    "                event = json.loads(json_str)\n",
    "                if event.get('event') != 'compaction_finished':\n",
    "                    continue\n",
    "                end_sec = event['time_micros']/1e6\n",
    "                start_sec = end_sec - event['compaction_time_micros']/1e6\n",
    "                read_rate = event['read_rate']\n",
    "                write_rate = event['write_rate']\n",
    "                level = event['output_level']\n",
    "                cf_name = event['cf_name']\n",
    "                compaction_data.setdefault(cf_name, []).append((start_sec, end_sec, read_rate, write_rate, level))\n",
    "            except Exception as e:\n",
    "                print(\"Compaction json error:\", e)\n",
    "\n",
    "# Convert timestamps to relative times (seconds since experiment start)\n",
    "l0_times = [ts/1e6 - experiment_start_time for ts, _ in l0_stalls]\n",
    "l0_rates = [rate for _, rate in l0_stalls]\n",
    "\n",
    "memtable_times = [ts/1e6 - experiment_start_time for ts, _ in memtable_stalls]\n",
    "memtable_rates = [rate for _, rate in memtable_stalls]\n",
    "\n",
    "pending_times = [ts/1e6 - experiment_start_time for ts, _ in pending_compaction_stalls]\n",
    "pending_rates = [rate for _, rate in pending_compaction_stalls]\n",
    "\n",
    "# Add stall events (as markers) on row 14\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=l0_times, y=l0_rates, mode='markers',\n",
    "    marker=dict(color='blue', size=6),\n",
    "    name=\"L0 Stalls\"\n",
    "), row=14, col=1)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=memtable_times, y=memtable_rates, mode='markers',\n",
    "    marker=dict(color='purple', size=6),\n",
    "    name=\"Memtable Stalls\"\n",
    "), row=14, col=1)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=pending_times, y=pending_rates, mode='markers',\n",
    "    marker=dict(color='orange', size=6),\n",
    "    name=\"Pending Compaction Stalls\"\n",
    "), row=14, col=1)\n",
    "\n",
    "# Add Flush events as horizontal lines per client\n",
    "for cf_name, events in flush_data.items():\n",
    "    base_color = get_client_base_color(cf_name)\n",
    "    for i, (start_sec, rate, duration) in enumerate(events):\n",
    "        dash_style = 'dash' if i == 0 else 'solid'\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[start_sec - experiment_start_time, (start_sec + duration) - experiment_start_time],\n",
    "            y=[rate, rate],\n",
    "            mode='lines',\n",
    "            line=dict(color=base_color, width=4, dash=dash_style),\n",
    "            name=f\"Client {cf_name} Flush\",\n",
    "            showlegend=True if i == 0 else False\n",
    "        ), row=14, col=1)\n",
    "\n",
    "# Add Compaction events as horizontal lines per event\n",
    "for cf_name, events in compaction_data.items():\n",
    "    for i, (start_sec, end_sec, read_rate, write_rate, level) in enumerate(events):\n",
    "        color = get_client_color(cf_name, level)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[start_sec - experiment_start_time, end_sec - experiment_start_time],\n",
    "            y=[write_rate, write_rate],\n",
    "            mode='lines',\n",
    "            line=dict(color=color, width=2),\n",
    "            name=f\"Client {cf_name} Compaction\",\n",
    "            showlegend=True if i == 0 else False\n",
    "        ), row=14, col=1)\n",
    "\n",
    "# Add Memtable Stops as vertical dashed lines (shapes)\n",
    "# (Shapes are global; here we set y0=0 and y1 a reasonable max)\n",
    "max_y = max(l0_rates + memtable_rates + pending_rates) if (l0_rates or memtable_rates or pending_rates) else 100\n",
    "for ts, cf_name in memtable_stops:\n",
    "    rel_time = ts/1e6 - experiment_start_time\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=rel_time, x1=rel_time,\n",
    "        y0=0, y1=max_y,\n",
    "        line=dict(color=\"brown\", width=1, dash=\"dash\"),\n",
    "        opacity=0.5\n",
    "    )\n",
    "# Dummy trace for legend of memtable stops\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None], y=[None],\n",
    "    mode='lines',\n",
    "    line=dict(color=\"brown\", width=1, dash=\"dash\"),\n",
    "    name=\"Memtable Stops\"\n",
    "), row=14, col=1)\n",
    "\n",
    "# ---------------------------\n",
    "# Final Layout Updates and Show Figure\n",
    "# ---------------------------\n",
    "fig.update_layout(height=3900, width=1000, showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "import json\n",
    "import colorsys\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "\n",
    "# ---------------------------\n",
    "# Flags and Filenames\n",
    "# ---------------------------\n",
    "\n",
    "def plot_data(load_from_pickle, save_pickle, pickle_filename):\n",
    "  # ---------------------------\n",
    "  # Data Loading / Scraping Section\n",
    "  # ---------------------------\n",
    "  if load_from_pickle:\n",
    "      with open(pickle_filename, 'rb') as f:\n",
    "          data = pickle.load(f)\n",
    "      # Extract dataframes and other variables from the pickled data\n",
    "      df = data['df']\n",
    "      df_th = data['df_th']\n",
    "      df_read = data['df_read']\n",
    "      df_insert = data['df_insert']\n",
    "      df_insert_batch = data['df_insert_batch']\n",
    "      df_queue = data['df_queue']\n",
    "      df_read_latency = data['df_read_latency']\n",
    "      df_insert_latency = data['df_insert_latency']\n",
    "      pivot_cache = data['pivot_cache']\n",
    "      df_iostat = data['df_iostat']\n",
    "      l0_stalls = data['l0_stalls']\n",
    "      memtable_stalls = data['memtable_stalls']\n",
    "      pending_compaction_stalls = data['pending_compaction_stalls']\n",
    "      memtable_stops = data['memtable_stops']\n",
    "      flush_data = data['flush_data']\n",
    "      compaction_data = data['compaction_data']\n",
    "      experiment_start_time = data['experiment_start_time']\n",
    "      time_seconds = data['time_seconds']\n",
    "  else:\n",
    "      # ---------------------------\n",
    "      # Scrape and Process Log Data\n",
    "      # ---------------------------\n",
    "      # Constants\n",
    "      READ_SIZE_MB = 64/1024\n",
    "      INSERT_SIZE_MB = 1/1024\n",
    "      INSERT_BATCH_SIZE_MB = 100/1024\n",
    "\n",
    "      # Load and preprocess main log\n",
    "      df = pd.read_csv('logs/client_stats.log')\n",
    "      df['relative_time_s'] = (df['timestamp'] - df['timestamp'].min()) / 1000\n",
    "      df = df.sort_values(['client_id', 'op_type', 'timestamp'])\n",
    "      df['time_diff_s'] = df.groupby(['client_id', 'op_type'])['relative_time_s'].diff()\n",
    "      df['throughput'] = df['count'] / df['time_diff_s']\n",
    "      df['throughput_mb_s'] = np.select(\n",
    "          [df.op_type=='READ', df.op_type=='INSERT', df.op_type=='INSERT_BATCH'],\n",
    "          [df.throughput * READ_SIZE_MB, df.throughput * INSERT_SIZE_MB, df.throughput * INSERT_BATCH_SIZE_MB],\n",
    "          default=np.nan\n",
    "      )\n",
    "      df_th = df.dropna(subset=['throughput_mb_s'])\n",
    "\n",
    "      # Split by op_type\n",
    "      df_read = df_th[df_th.op_type=='READ'].copy()\n",
    "      df_insert = df_th[df_th.op_type=='INSERT']\n",
    "      df_insert_batch = df_th[df_th.op_type=='INSERT_BATCH']\n",
    "      df_queue = df[df.op_type=='QUEUE']\n",
    "      df_read_latency = df[df.op_type=='READ']\n",
    "      df_insert_latency = df[df.op_type=='INSERT']\n",
    "\n",
    "      # Cache metrics\n",
    "      df_read['user_cache_total'] = df_read.user_cache_hits + df_read.user_cache_misses\n",
    "      df_read['user_cache_hit_rate'] = np.where(\n",
    "          df_read.user_cache_total > 0,\n",
    "          df_read.user_cache_hits / df_read.user_cache_total * 100,\n",
    "          np.nan\n",
    "      )\n",
    "      df_cache = df_read[['relative_time_s', 'client_id', 'user_cache_usage']].dropna()\n",
    "      pivot_cache = df_cache.pivot_table(index='relative_time_s',\n",
    "                                        columns='client_id',\n",
    "                                        values='user_cache_usage',\n",
    "                                        aggfunc='mean').fillna(0)\n",
    "\n",
    "      # Load iostat data\n",
    "      df_iostat = pd.read_csv(\"iostat_results.csv\")\n",
    "      time_seconds = np.arange(len(df_iostat)) + df['relative_time_s'].min()\n",
    "\n",
    "      # ---------------------------\n",
    "      # Parse RocksDB Logs\n",
    "      # ---------------------------\n",
    "      experiment_start_time = 0\n",
    "\n",
    "      # Regex patterns and helper functions\n",
    "      flush_regex = re.compile(\n",
    "          r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\d+ \\[/flush_job\\.cc:\\d+\\] \\[(.*?)\\] \\[JOB \\d+\\] Flush: (\\d+) microseconds, \\d+ cpu microseconds, (\\d+) bytes'\n",
    "      )\n",
    "      l0_stall_pattern = re.compile(\n",
    "          r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\d+ \\[WARN\\] \\[/column_family.cc:\\d+\\] \\[([^,]+)\\] Stalling writes because we have \\d+ level-0 files rate (\\d+)'\n",
    "      )\n",
    "      memtable_stall_pattern = re.compile(\n",
    "          r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\d+ \\[WARN\\] \\[/column_family.cc:\\d+\\] \\[([^,]+)\\] Stalling writes because we have \\d+ immutable memtables.*rate (\\d+)'\n",
    "      )\n",
    "      pending_compaction_stall_pattern = re.compile(\n",
    "          r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\d+ \\[WARN\\] \\[/column_family.cc:\\d+\\] \\[([^,]+)\\] Stalling writes because of estimated pending compaction bytes \\d+ rate (\\d+)'\n",
    "      )\n",
    "      memtable_stop_pattern = re.compile(\n",
    "          r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\d+ \\[WARN\\] \\[/column_family.cc:\\d+\\] \\[([^,]+)\\] Stopping writes because we have \\d+ immutable memtables.*'\n",
    "      )\n",
    "      compaction_regex = re.compile(r'.*EVENT_LOG_v1 (.*)$')\n",
    "\n",
    "      def timestamp_to_seconds(timestamp_str):\n",
    "          ts = datetime.strptime(timestamp_str, '%Y/%m/%d-%H:%M:%S.%f')\n",
    "          epoch = datetime(1970, 1, 1)\n",
    "          return (ts - epoch).total_seconds()\n",
    "\n",
    "      def timestamp_to_micros(timestamp_str):\n",
    "          dt = datetime.strptime(timestamp_str, '%Y/%m/%d-%H:%M:%S.%f')\n",
    "          epoch = datetime(1970, 1, 1)\n",
    "          return int((dt - epoch).total_seconds() * 1e6)\n",
    "\n",
    "      # Containers for events\n",
    "      l0_stalls = []\n",
    "      memtable_stalls = []\n",
    "      pending_compaction_stalls = []\n",
    "      compaction_data = {}  # keyed by client (cf_name)\n",
    "      flush_data = {}       # keyed by client\n",
    "      memtable_stops = []\n",
    "\n",
    "      log_file_path = '/mnt/rocksdb/ycsb-rocksdb-data/LOG'\n",
    "      with open(log_file_path, 'r') as log_file:\n",
    "          for line in log_file:\n",
    "              m = l0_stall_pattern.search(line)\n",
    "              if m:\n",
    "                  timestamp_str, cf_name, rate = m.groups()\n",
    "                  ts = timestamp_to_micros(timestamp_str)\n",
    "                  l0_stalls.append((ts, int(rate)/1024/1024))\n",
    "              m = memtable_stall_pattern.search(line)\n",
    "              if m:\n",
    "                  timestamp_str, cf_name, rate = m.groups()\n",
    "                  ts = timestamp_to_micros(timestamp_str)\n",
    "                  memtable_stalls.append((ts, int(rate)/1024/1024))\n",
    "              m = memtable_stop_pattern.search(line)\n",
    "              if m:\n",
    "                  timestamp_str, cf_name = m.groups()\n",
    "                  ts = timestamp_to_micros(timestamp_str)\n",
    "                  memtable_stops.append((ts, cf_name))\n",
    "              m = pending_compaction_stall_pattern.search(line)\n",
    "              if m:\n",
    "                  timestamp_str, cf_name, rate = m.groups()\n",
    "                  ts = timestamp_to_micros(timestamp_str)\n",
    "                  pending_compaction_stalls.append((ts, int(rate)/1024/1024))\n",
    "              m = flush_regex.match(line)\n",
    "              if m:\n",
    "                  timestamp_str, cf_name, flush_micro, flush_bytes = m.groups()\n",
    "                  start_sec = timestamp_to_seconds(timestamp_str) - int(flush_micro)/1e6\n",
    "                  rate = (int(flush_bytes) / int(flush_micro)) * 1e6 / (1024**2)\n",
    "                  flush_data.setdefault(cf_name, []).append((start_sec, rate, int(flush_micro)/1e6))\n",
    "              m = compaction_regex.match(line)\n",
    "              if m:\n",
    "                  json_str = m.group(1)\n",
    "                  try:\n",
    "                      event = json.loads(json_str)\n",
    "                      if event.get('event') != 'compaction_finished':\n",
    "                          continue\n",
    "                      end_sec = event['time_micros']/1e6\n",
    "                      start_sec = end_sec - event['compaction_time_micros']/1e6\n",
    "                      read_rate = event['read_rate']\n",
    "                      write_rate = event['write_rate']\n",
    "                      level = event['output_level']\n",
    "                      cf_name = event['cf_name']\n",
    "                      compaction_data.setdefault(cf_name, []).append((start_sec, end_sec, read_rate, write_rate, level))\n",
    "                  except Exception as e:\n",
    "                      print(\"Compaction json error:\", e)\n",
    "\n",
    "      # Adjust timestamps to be relative to experiment start time\n",
    "      l0_stalls = [(ts, rate) for ts, rate in l0_stalls]\n",
    "      memtable_stalls = [(ts, rate) for ts, rate in memtable_stalls]\n",
    "      pending_compaction_stalls = [(ts, rate) for ts, rate in pending_compaction_stalls]\n",
    "      # Convert to relative times (seconds) for plotting\n",
    "      l0_times = [ts/1e6 - experiment_start_time for ts, _ in l0_stalls]\n",
    "      l0_rates = [rate for _, rate in l0_stalls]\n",
    "      memtable_times = [ts/1e6 - experiment_start_time for ts, _ in memtable_stalls]\n",
    "      memtable_rates = [rate for _, rate in memtable_stalls]\n",
    "      pending_times = [ts/1e6 - experiment_start_time for ts, _ in pending_compaction_stalls]\n",
    "      pending_rates = [rate for _, rate in pending_compaction_stalls]\n",
    "\n",
    "      # ---------------------------\n",
    "      # Optionally Pickle the Data for Future Use\n",
    "      # ---------------------------\n",
    "      if save_pickle:\n",
    "          data_to_pickle = {\n",
    "              'df': df,\n",
    "              'df_th': df_th,\n",
    "              'df_read': df_read,\n",
    "              'df_insert': df_insert,\n",
    "              'df_insert_batch': df_insert_batch,\n",
    "              'df_queue': df_queue,\n",
    "              'df_read_latency': df_read_latency,\n",
    "              'df_insert_latency': df_insert_latency,\n",
    "              'pivot_cache': pivot_cache,\n",
    "              'df_iostat': df_iostat,\n",
    "              'l0_stalls': l0_stalls,\n",
    "              'memtable_stalls': memtable_stalls,\n",
    "              'pending_compaction_stalls': pending_compaction_stalls,\n",
    "              'memtable_stops': memtable_stops,\n",
    "              'flush_data': flush_data,\n",
    "              'compaction_data': compaction_data,\n",
    "              'experiment_start_time': experiment_start_time,\n",
    "              'time_seconds': time_seconds\n",
    "          }\n",
    "          with open(pickle_filename, 'wb') as f:\n",
    "              pickle.dump(data_to_pickle, f)\n",
    "          print(f\"Plot data pickled to {pickle_filename}\")\n",
    "\n",
    "  # ---------------------------\n",
    "  # Plotting Section (Same for both data sources)\n",
    "  # ---------------------------\n",
    "  # Create subplots (14 rows; row 14 for RocksDB events)\n",
    "  fig = make_subplots(rows=14, cols=1, shared_xaxes=True, vertical_spacing=0.01,\n",
    "                      subplot_titles=[\n",
    "                          f\"Per-Client READ Throughput for read size {64:.0f}KB\",\n",
    "                          \"Per-Client WRITE Throughput (INSERT & INSERT_BATCH)\",\n",
    "                          \"Combined READ & WRITE Throughput\",\n",
    "                          \"Per-Client READ P10 Latency\", \"Per-Client READ P25 Latency\",\n",
    "                          \"Per-Client READ P50 Latency\", \"Per-Client READ P99 Latency\",\n",
    "                          \"Per-Client READ Max Latency\", \"Per-Client INSERT P99 Latency\",\n",
    "                          \"Per-Client QUEUE P99 Latency\", \"Per-Client User Cache Hit Rate\",\n",
    "                          \"Per-Client User Cache Usage (Stacked)\",\n",
    "                          \"SSD Throughput Over Time\",\n",
    "                          \"RocksDB Events Over Time\"\n",
    "                      ])\n",
    "\n",
    "  def add_group(df_group, row, ycol, name_fmt):\n",
    "      for cid, grp in df_group.groupby('client_id'):\n",
    "          fig.add_trace(go.Scatter(x=grp.relative_time_s, y=grp[ycol], name=name_fmt.format(cid)),\n",
    "                        row=row, col=1)\n",
    "\n",
    "  # Subplots 1–3 and 11–12 (using dataframes)\n",
    "  add_group(df_read, 1, 'throughput_mb_s', \"Client {}\")\n",
    "  add_group(df_insert, 2, 'throughput_mb_s', \"Client {} INSERT\")\n",
    "  add_group(df_insert_batch, 2, 'throughput_mb_s', \"Client {} INSERT_BATCH\")\n",
    "  for (cid, op), grp in df_th.groupby(['client_id', 'op_type']):\n",
    "      fig.add_trace(go.Scatter(x=grp.relative_time_s, y=grp.throughput_mb_s,\n",
    "                              name=f\"Client {cid}, {op}\"), row=3, col=1)\n",
    "  for col_name, row_num in [('10p',4), ('25p',5), ('50p',6), ('99p',7), ('max',8)]:\n",
    "      add_group(df_read_latency, row_num, col_name, \"Client {}\")\n",
    "  add_group(df_insert_latency, 9, '99p', \"Client {}\")\n",
    "  add_group(df_queue, 10, '99p', \"Client {}\")\n",
    "  add_group(df_read, 11, 'user_cache_hit_rate', \"Client {}\")\n",
    "  for cid in pivot_cache.columns:\n",
    "      fig.add_trace(go.Scatter(x=pivot_cache.index, y=pivot_cache[cid]/1024**2,\n",
    "                              name=f\"Client {cid}\", stackgroup=\"one\"), row=12, col=1)\n",
    "\n",
    "  # SSD Throughput subplot (row 13)\n",
    "  fig.add_trace(go.Scatter(x=time_seconds, y=df_iostat[\"rMB/s\"].iloc[:len(df_iostat)],\n",
    "                          name=\"SSD Read MB/s\", mode=\"lines+markers\"), row=13, col=1)\n",
    "  fig.add_trace(go.Scatter(x=time_seconds, y=df_iostat[\"wMB/s\"].iloc[:len(df_iostat)],\n",
    "                          name=\"SSD Write MB/s\", mode=\"lines+markers\"), row=13, col=1)\n",
    "\n",
    "  # Axis labels and formatting\n",
    "  y_labels = ['Throughput (MB/s)']*3 + ['Latency (ms)']*7 + ['Hit Rate (%)','Cache Usage (MB)','MB/s', '']\n",
    "  for r, label in enumerate(y_labels, start=1):\n",
    "      fig.update_yaxes(title_text=label, row=r, col=1)\n",
    "  fig.update_xaxes(title_text=\"Time (s)\", row=14, col=1)\n",
    "  fig.update_yaxes(range=[0,400], row=5, col=1)\n",
    "\n",
    "  # ---------------------------\n",
    "  # RocksDB Events Plot (Row 14)\n",
    "  # ---------------------------\n",
    "  # Helper functions for client coloring\n",
    "  base_colors = {}\n",
    "\n",
    "  def get_client_base_color(client):\n",
    "      if client not in base_colors:\n",
    "          palette = px.colors.qualitative.Dark24\n",
    "          index = len(base_colors) % len(palette)\n",
    "          base_colors[client] = palette[index]\n",
    "      return base_colors[client]\n",
    "\n",
    "  def shade_color(base_color, level):\n",
    "      r, g, b = [int(base_color.lstrip('#')[i:i+2], 16)/255.0 for i in (0, 2, 4)]\n",
    "      h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "      new_l = 0.9 - (level - 1) * ((0.9 - 0.5) / 6)\n",
    "      new_r, new_g, new_b = colorsys.hls_to_rgb(h, new_l, s)\n",
    "      return '#%02x%02x%02x' % (int(new_r*255), int(new_g*255), int(new_b*255))\n",
    "\n",
    "  def get_client_color(client, level):\n",
    "      base = get_client_base_color(client)\n",
    "      return shade_color(base, level)\n",
    "\n",
    "  # L0, memtable, and pending compaction events as markers\n",
    "  l0_times = [ts/1e6 - experiment_start_time for ts, _ in l0_stalls]\n",
    "  l0_rates = [rate for _, rate in l0_stalls]\n",
    "  memtable_times = [ts/1e6 - experiment_start_time for ts, _ in memtable_stalls]\n",
    "  memtable_rates = [rate for _, rate in memtable_stalls]\n",
    "  pending_times = [ts/1e6 - experiment_start_time for ts, _ in pending_compaction_stalls]\n",
    "  pending_rates = [rate for _, rate in pending_compaction_stalls]\n",
    "\n",
    "  fig.add_trace(go.Scatter(\n",
    "      x=l0_times, y=l0_rates, mode='markers',\n",
    "      marker=dict(color='blue', size=6),\n",
    "      name=\"L0 Stalls\"\n",
    "  ), row=14, col=1)\n",
    "  fig.add_trace(go.Scatter(\n",
    "      x=memtable_times, y=memtable_rates, mode='markers',\n",
    "      marker=dict(color='purple', size=6),\n",
    "      name=\"Memtable Stalls\"\n",
    "  ), row=14, col=1)\n",
    "  fig.add_trace(go.Scatter(\n",
    "      x=pending_times, y=pending_rates, mode='markers',\n",
    "      marker=dict(color='orange', size=6),\n",
    "      name=\"Pending Compaction Stalls\"\n",
    "  ), row=14, col=1)\n",
    "\n",
    "  # Flush events as horizontal lines\n",
    "  for cf_name, events in flush_data.items():\n",
    "      base_color = get_client_base_color(cf_name)\n",
    "      for i, (start_sec, rate, duration) in enumerate(events):\n",
    "          dash_style = 'dash' if i == 0 else 'solid'\n",
    "          fig.add_trace(go.Scatter(\n",
    "              x=[start_sec - experiment_start_time, (start_sec + duration) - experiment_start_time],\n",
    "              y=[rate, rate],\n",
    "              mode='lines',\n",
    "              line=dict(color=base_color, width=4, dash=dash_style),\n",
    "              name=f\"Client {cf_name} Flush\",\n",
    "              showlegend=True if i == 0 else False\n",
    "          ), row=14, col=1)\n",
    "\n",
    "  # Compaction events as horizontal lines\n",
    "  for cf_name, events in compaction_data.items():\n",
    "      for i, (start_sec, end_sec, read_rate, write_rate, level) in enumerate(events):\n",
    "          color = get_client_color(cf_name, level)\n",
    "          fig.add_trace(go.Scatter(\n",
    "              x=[start_sec - experiment_start_time, end_sec - experiment_start_time],\n",
    "              y=[write_rate, write_rate],\n",
    "              mode='lines',\n",
    "              line=dict(color=color, width=2),\n",
    "              name=f\"Client {cf_name} Compaction\",\n",
    "              showlegend=True if i == 0 else False\n",
    "          ), row=14, col=1)\n",
    "\n",
    "  # Memtable stops as vertical dashed lines\n",
    "  max_y = max(l0_rates + memtable_rates + pending_rates) if (l0_rates or memtable_rates or pending_rates) else 100\n",
    "  for ts, cf_name in memtable_stops:\n",
    "      rel_time = ts/1e6 - experiment_start_time\n",
    "      fig.add_shape(\n",
    "          type=\"line\",\n",
    "          x0=rel_time, x1=rel_time,\n",
    "          y0=0, y1=max_y,\n",
    "          line=dict(color=\"brown\", width=1, dash=\"dash\"),\n",
    "          opacity=0.5\n",
    "      )\n",
    "  # Dummy trace for memtable stops legend\n",
    "  fig.add_trace(go.Scatter(\n",
    "      x=[None], y=[None],\n",
    "      mode='lines',\n",
    "      line=dict(color=\"brown\", width=1, dash=\"dash\"),\n",
    "      name=\"Memtable Stops\"\n",
    "  ), row=14, col=1)\n",
    "\n",
    "  fig.update_layout(height=3900, width=1000, showlegend=False)\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_FROM_PICKLE = False   # Set to True to load data from pickle file instead of scraping logs\n",
    "SAVE_PICKLE = False         # Set to True to pickle scraped data for future use\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "PICKLE_FILENAME = f'pickles/plot_data_{timestamp}.pkl'\n",
    "# PICKLE_FILENAME = 'pickles/plot_data_20250402_192557.pkl'\n",
    "\n",
    "plot_data(LOAD_FROM_PICKLE, SAVE_PICKLE, PICKLE_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
