{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib \n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "stat_dump_interval_ms = 100\n",
    "stat_dump_interval_s = stat_dump_interval_ms / 1000\n",
    "\n",
    "def plot_client_results(output_file, axs, xlim):\n",
    "    # Define the file path\n",
    "    input_file_path = 'logs/client_stats.log'\n",
    "\n",
    "    client_data = {}\n",
    "    start_time = None\n",
    "\n",
    "    # Open and read the CSV file\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip the header row\n",
    "\n",
    "        for row in reader:\n",
    "            timestamp_str, client_id, op_type, count, max_val, min_val, avg, p50, p90, p99, p999, _, _, _, _, _, _, _, _ = row\n",
    "            timestamp_s = int(timestamp_str) / 1000\n",
    "            if start_time is None:\n",
    "                start_time = timestamp_s\n",
    "\n",
    "            if client_id not in client_data:\n",
    "                client_data[client_id] = {'Tput': [], '50': [], '99': [], '999': [], 'ts': []}\n",
    "\n",
    "            multiplier = 1\n",
    "            if op_type == 'INSERT_BATCH':\n",
    "                multiplier = 86\n",
    "            elif op_type == 'SCAN':\n",
    "                multiplier = 100\n",
    "\n",
    "            client_data[client_id]['Tput'].append(multiplier * int(count) * 1024 / (1024 * 1024) / stat_dump_interval_s)\n",
    "            client_data[client_id]['50'].append(float(p50) / 1000)\n",
    "            client_data[client_id]['99'].append(float(p99) / 1000)\n",
    "            client_data[client_id]['999'].append(float(p999) / 1000)\n",
    "            client_data[client_id]['ts'].append(timestamp_s)\n",
    "\n",
    "    def plot_metric(metric, title, ylabel, fig_loc, ylim):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        for client_id, stats in client_data.items():\n",
    "            time_points = [ts - start_time for ts in client_data[client_id]['ts']]\n",
    "            data_points = [x for x in stats[metric]]\n",
    "            axs[fig_loc[0]].plot(time_points, data_points, marker='o', label=f'Client {client_id}')\n",
    "            with open(output_file, 'a') as outfile:\n",
    "                outfile.write(f\"client-{client_id}:metric-{metric}\\n\")\n",
    "                outfile.write(f\"time_points:{time_points}\\n\")\n",
    "                outfile.write(f\"data_points:{data_points}\\n\")\n",
    "\n",
    "        if metric == 'Tput':\n",
    "            # Get all the time points:\n",
    "            all_time_points = [ts for client_id in client_data for ts in client_data[client_id]['ts']]\n",
    "            all_time_points = sorted(list(set(all_time_points)))\n",
    "\n",
    "            # For each time point, sum the client throughputs.\n",
    "            client_tput_sums = []\n",
    "            for ts in all_time_points:\n",
    "                cur_sum = 0\n",
    "                for client_id in client_data:\n",
    "                    if ts in client_data[client_id]['ts']:\n",
    "                        idx = client_data[client_id]['ts'].index(ts)\n",
    "                        cur_sum += client_data[client_id]['Tput'][idx]\n",
    "                client_tput_sums.append(cur_sum)\n",
    "            all_time_points = [ts - start_time for ts in all_time_points]\n",
    "            # axs[fig_loc[0]].plot(all_time_points, client_tput_sums, marker='o', label='Sum')\n",
    "\n",
    "        axs[fig_loc[0]].set_title(title)\n",
    "        axs[fig_loc[0]].set_xlabel('Time (s)')\n",
    "        axs[fig_loc[0]].set_xlim(xlim)\n",
    "        # axs[fig_loc[0]].set_ylim(0, 40)\n",
    "        axs[fig_loc[0]].set_ylabel(ylabel)\n",
    "        axs[fig_loc[0]].legend()\n",
    "        axs[fig_loc[0]].grid(True)\n",
    "        # axs[fig_loc[0]].set_ylim(-2, ylim)\n",
    "\n",
    "    # Plotting each metric\n",
    "    plot_metric('Tput', 'Client Throughput', 'MB/s', (0, 0), 550)\n",
    "    plot_metric('50', 'Latency: 50p', '(ms)', (1, 0), 25)\n",
    "    plot_metric('99', 'Latency: 99p', '99th Percentile (ms)', (2, 0), 2000)\n",
    "    return start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def plot_memtable_stats(output_file, axs, start_time, xlim, fig_loc):\n",
    "    # Define the file paths\n",
    "    input_file_path = '/mnt/rocksdb/ycsb-rocksdb-data/LOG'\n",
    "\n",
    "    cf_data = {}\n",
    "\n",
    "    # Compile the regular expressions for matching lines\n",
    "    \n",
    "    # memtable_regex = re.compile(r'^memtables,([^,]+),(\\d+),(\\d+)MB$')\n",
    "    # memtable_regex = re.compile(r'^memtables,([^,]+),(\\d+),(\\d+)MB$')\n",
    "    memtable_pattern = re.compile(r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}).\\d{6} \\d+ .*.cc:\\d+\\] mt,([^,]+),([^,]+).*')\n",
    "\n",
    "\n",
    "    def timestamp_to_seconds(timestamp_str):\n",
    "        timestamp_str = timestamp_str.strip()\n",
    "        timestamp = datetime.strptime(timestamp_str, '%Y/%m/%d-%H:%M:%S')\n",
    "        epoch = datetime(1970, 1, 1)\n",
    "        return (timestamp - epoch).total_seconds()\n",
    "\n",
    "    # Open and read the input file\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            memtable_match = memtable_pattern.match(line)\n",
    "            if not memtable_match:\n",
    "                continue\n",
    "                \n",
    "            ts, cf_name, operation = memtable_match.group(1, 2, 3)\n",
    "            operation = operation.strip()\n",
    "            if cf_name not in cf_data:\n",
    "                # Assume 0 active memtables to start. Thus, this is basically tracking imm tables pending flush\n",
    "                cf_data[cf_name] = {\"counts\" : [0], \"ts\" : [start_time]}\n",
    "            cf_data[cf_name][\"ts\"].append(timestamp_to_seconds(ts))\n",
    "            if operation == 'add':\n",
    "                cf_data[cf_name][\"counts\"].append(cf_data[cf_name][\"counts\"][-1] + 1)\n",
    "            elif operation == 'remove':\n",
    "                cf_data[cf_name][\"counts\"].append(cf_data[cf_name][\"counts\"][-1] - 1)\n",
    "            else:\n",
    "                print('invalid memtable operation name')\n",
    "                \n",
    "                \n",
    "    def plot_metric(metric, ax, colors):\n",
    "        idx = 0\n",
    "        for cf_name, stats in cf_data.items():\n",
    "            data_points = [int(x) for x in stats[metric]]\n",
    "            time_points = [ts - start_time for ts in stats[\"ts\"]]\n",
    "\n",
    "            # Ignore initial memtable init.\n",
    "            # ax.plot(time_points, data_points, marker='o', label=f'{cf_name}', color=colors[idx])\n",
    "            if cf_name == 'default':\n",
    "                label = 'Client 0'\n",
    "            else:\n",
    "                label = 'Client ' + cf_name[-1]\n",
    "            ax.plot(time_points, data_points, marker='o', label=f'{label}')\n",
    "            with open(output_file, 'a') as outfile:\n",
    "                outfile.write(f\"cf-{cf_name}:metric-{metric}\\n\")\n",
    "                outfile.write(f\"time_points:{time_points}\\n\")\n",
    "                outfile.write(f\"data_points:{data_points}\\n\")\n",
    "\n",
    "\n",
    "            idx += 1\n",
    "        ax.set_xlabel('Time (s)')\n",
    "            \n",
    "    # Plotting each metric\n",
    "    plot_metric('counts', axs[fig_loc[0]], ['blue', 'lightblue'])\n",
    "    # plot_metric('counts', axs[fig_loc[0]], ['orange', '#ffd580'])\n",
    "    axs[fig_loc[0]].set_xlim(xlim)\n",
    "    axs[fig_loc[0]].set_title(\"Memtable Stats\")\n",
    "    axs[fig_loc[0]].legend(loc='upper left')\n",
    "    axs[fig_loc[0]].grid(True)\n",
    "    axs[fig_loc[0]].set_ylabel('Count')\n",
    "\n",
    "    # size_ax = axs[fig_loc[0]].twinx()\n",
    "    # plot_metric('table_sizes', size_ax, ['blue', 'lightblue'])\n",
    "    # size_ax.legend(loc='upper right')\n",
    "    # size_ax.set_ylabel('Memtables Size (MB)')\n",
    "    # size_ax.set_ylim(0,128)\n",
    "    # plt.subplots_adjust(right=0.85)\n",
    "\n",
    "def plot_level_stats(output_file, axs, fig_loc):\n",
    "    # Define the file paths\n",
    "    input_file_path = '/mnt/rocksdb/ycsb-rocksdb-data/LOG'\n",
    "\n",
    "    level_data = {}\n",
    "    level_hit_pattern = re.compile(r'rocksdb.l(0|1|2|3).hit COUNT : (\\d+).*')\n",
    "\n",
    "    hits = [None, None, None, None]\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            level_hit_match = level_hit_pattern.match(line)\n",
    "            if not level_hit_match:\n",
    "                continue\n",
    "            level, new_hits = level_hit_match.group(1, 2)\n",
    "            # Always overwrite.\n",
    "            hits[int(level)] = int(new_hits)\n",
    "\n",
    "    cumulative_hits = []\n",
    "    cumulative_count = 0\n",
    "    for hit in hits:\n",
    "        cumulative_count += hit\n",
    "        cumulative_hits.append(cumulative_count)\n",
    "    cumulative_hits_prop = [x / cumulative_count * 100 for x in cumulative_hits]\n",
    "    \n",
    "    axs[fig_loc[0]].plot([\"l0\", \"l1\", \"l2\", \"l3\"], cumulative_hits_prop, marker='o')\n",
    "\n",
    "    axs[fig_loc[0]].set_title(\"Level Hits CDF\")\n",
    "    axs[fig_loc[0]].grid(True)\n",
    "    axs[fig_loc[0]].set_ylabel('Cumulative Hits (%)')\n",
    "\n",
    "    with open(output_file, 'a') as outfile:\n",
    "        outfile.write(f\"metric-level_hits\\n\")\n",
    "        outfile.write(f\"data_points:{cumulative_hits_prop}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_compaction_color(cf_name, level):\n",
    "    # Red\n",
    "    if cf_name == \"default\":\n",
    "        if level == 1:\n",
    "            return \"#fc9598\"\n",
    "        elif level == 2:\n",
    "            return \"#ff696e\"\n",
    "        elif level == 3:\n",
    "            return \"#f02225\"\n",
    "        elif level == 4:\n",
    "            return \"#cf1d20\"\n",
    "        elif level == 5:\n",
    "            return \"#ab1619\"\n",
    "        elif level == 6:\n",
    "            return \"#851114\"\n",
    "        else:\n",
    "        # elif level == 7:\n",
    "            return \"#610c0e\"\n",
    "    # Blue\n",
    "    elif cf_name == \"cf2\":\n",
    "        if level == 1:\n",
    "            return \"#95c2fc\"\n",
    "        elif level == 2:\n",
    "            return \"#699eff\"\n",
    "        elif level == 3:\n",
    "            return \"#225bf0\"\n",
    "        elif level == 4:\n",
    "            return \"#1d4bcf\"\n",
    "        elif level == 5:\n",
    "            return \"#1638ab\"\n",
    "        elif level == 6:\n",
    "            return \"#112985\"\n",
    "        else:\n",
    "            # elif level == 7:\n",
    "            return \"#0c1d61\"\n",
    "    # Orange\n",
    "    elif cf_name == \"cf3\":\n",
    "        if level == 1:\n",
    "            return \"#ffd1b3\"  # light orange\n",
    "        elif level == 2:\n",
    "            return \"#ffa366\"  # medium light orange\n",
    "        elif level == 3:\n",
    "            return \"#ff7519\"  # medium orange\n",
    "        elif level == 4:\n",
    "            return \"#cc5c14\"  # medium dark orange\n",
    "        elif level == 5:\n",
    "            return \"#993d0f\"  # dark orange\n",
    "        elif level == 6:\n",
    "            return \"#66260a\"  # darker orange\n",
    "        else:\n",
    "            # elif level == 7:\n",
    "            return \"#331305\"  # darkest orange\n",
    "    else:\n",
    "        if level == 1:\n",
    "            return \"#c9fcb2\"  # light green\n",
    "        elif level == 2:\n",
    "            return \"#9ff987\"  # medium light green\n",
    "        elif level == 3:\n",
    "            return \"#75f35d\"  # medium green\n",
    "        elif level == 4:\n",
    "            return \"#58d740\"  # medium dark green\n",
    "        elif level == 5:\n",
    "            return \"#46b334\"  # dark green\n",
    "        elif level == 6:\n",
    "            return \"#328028\"  # darker green\n",
    "        else:\n",
    "            # elif level == 7:\n",
    "            return \"#21571d\"  # darkest green\n",
    "\n",
    "\n",
    "def plot_rocksdb_events(output_file, axs, experiment_start_time, xlim, fig_loc):\n",
    "    # log_file_path = '/mnt/multi-cf2/ycsb-rocksdb-data-2/LOG'\n",
    "    # log_file_path = '/mnt/multi-cf/ycsb-rocksdb-data/LOG'\n",
    "    log_file_path = '/mnt/rocksdb/ycsb-rocksdb-data/LOG'\n",
    "    flush_regex = re.compile(\n",
    "        r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\d+ \\[/flush_job\\.cc:\\d+\\] \\[(.*?)\\] \\[JOB \\d+\\] Flush: (\\d+) microseconds, \\d+ cpu microseconds, (\\d+) bytes'\n",
    "    )\n",
    "    l0_stall_pattern = re.compile(r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}.\\d{6}) \\d+ \\[WARN\\] \\[/column_family.cc:\\d+\\] \\[([^,]+)\\] Stalling writes because we have \\d+ level-0 files rate (\\d+)')\n",
    "    memtable_stall_pattern = re.compile(r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}.\\d{6}) \\d+ \\[WARN\\] \\[/column_family.cc:\\d+\\] \\[([^,]+)\\] Stalling writes because we have \\d+ immutable memtables.*rate (\\d+)')\n",
    "    pending_compaction_stall_pattern = re.compile(r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}.\\d{6}) \\d+ \\[WARN\\] \\[/column_family.cc:\\d+\\] \\[([^,]+)\\] Stalling writes because of estimated pending compaction bytes \\d+ rate (\\d+)')\n",
    "    memtable_stop_pattern = re.compile(r'(\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}.\\d{6}) \\d+ \\[WARN\\] \\[/column_family.cc:\\d+\\] \\[([^,]+)\\] Stopping writes because we have \\d+ immutable memtables.*')\n",
    "\n",
    "    compaction_regex = re.compile(r'.*EVENT_LOG_v1 (.*)$')\n",
    "    def timestamp_to_seconds(timestamp_str):\n",
    "        timestamp = datetime.strptime(timestamp_str, '%Y/%m/%d-%H:%M:%S.%f')\n",
    "        epoch = datetime(1970, 1, 1)\n",
    "        return (timestamp - epoch).total_seconds()\n",
    "\n",
    "    def timestamp_to_micros(timestamp_str):\n",
    "        timestamp_format = '%Y/%m/%d-%H:%M:%S.%f'\n",
    "        dt = datetime.strptime(timestamp_str, timestamp_format)\n",
    "        epoch = datetime(1970, 1, 1)\n",
    "        micros_since_epoch = int((dt - epoch).total_seconds() * 1000000)\n",
    "        return micros_since_epoch\n",
    "\n",
    "    # Initialize lists for all events\n",
    "    l0_stalls, memtable_stalls, pending_compaction_stalls = [], [], []\n",
    "    compaction_data = {}\n",
    "    flush_data = {}\n",
    "    memtable_stops = []\n",
    "\n",
    "    # Process the log file for stall, flush, and compaction events\n",
    "    with open(log_file_path, 'r') as log_file:\n",
    "        for line in log_file:\n",
    "            # L0 and Memtable Stalls\n",
    "            l0_match = l0_stall_pattern.search(line)\n",
    "            if l0_match:\n",
    "                timestamp_str, cf_name, rate = l0_match.groups()\n",
    "                timestamp_micros = timestamp_to_micros(timestamp_str)\n",
    "                l0_stalls.append((timestamp_micros, int(rate) / 1024 / 1024))\n",
    "\n",
    "            memtable_match = memtable_stall_pattern.search(line)\n",
    "            if memtable_match:\n",
    "                timestamp_str, cf_name, rate = memtable_match.groups()\n",
    "                timestamp_micros = timestamp_to_micros(timestamp_str)\n",
    "                memtable_stalls.append((timestamp_micros, int(rate) / 1024 / 1024))\n",
    "\n",
    "            memtable_stop_match = memtable_stop_pattern.search(line)\n",
    "            if memtable_stop_match:\n",
    "                timestamp_str, cf_name = memtable_stop_match.groups()\n",
    "                timestamp_micros = timestamp_to_micros(timestamp_str)\n",
    "                memtable_stops.append((timestamp_micros, cf_name))\n",
    "\n",
    "            pending_compact_match = pending_compaction_stall_pattern.search(line)\n",
    "            if pending_compact_match:\n",
    "                timestamp_str, cf_name, rate = pending_compact_match.groups()\n",
    "                timestamp_micros = timestamp_to_micros(timestamp_str)\n",
    "                pending_compaction_stalls.append((timestamp_micros, int(rate) / 1024 / 1024))\n",
    "\n",
    "            # Flush Events\n",
    "            flush_match = flush_regex.match(line)\n",
    "            if flush_match:\n",
    "                timestamp_str, cf_name, flush_microseconds, flush_bytes = flush_match.groups()\n",
    "                start_time_seconds = timestamp_to_seconds(timestamp_str) - int(flush_microseconds) / 1e6\n",
    "                rate_MB_s = (int(flush_bytes) / int(flush_microseconds)) * 1e6 / (1024**2)\n",
    "                if cf_name not in flush_data:\n",
    "                    flush_data[cf_name] = []\n",
    "                flush_data[cf_name].append((start_time_seconds, rate_MB_s, int(flush_microseconds)/1e6))\n",
    "            \n",
    "            # Compaction Events\n",
    "            compaction_match = compaction_regex.match(line)\n",
    "            if compaction_match:\n",
    "                json_str = compaction_match.group(1)\n",
    "                try:\n",
    "                    event_data = json.loads(json_str)\n",
    "                    if event_data['event'] != 'compaction_finished':\n",
    "                        continue\n",
    "                    end_time_seconds = event_data['time_micros'] / 1e6\n",
    "                    start_time_seconds = end_time_seconds - event_data['compaction_time_micros'] / 1e6\n",
    "                    # start_time_seconds = event_data['time_micros'] / 1e6\n",
    "                    # end_time_seconds = start_time_seconds + event_data['compaction_time_micros'] / 1e6\n",
    "                    read_rate = event_data['read_rate']\n",
    "                    write_rate = event_data['write_rate']\n",
    "                    output_level = event_data['output_level']\n",
    "                    cf_name = event_data['cf_name']\n",
    "                    if cf_name not in compaction_data:\n",
    "                        compaction_data[cf_name] = []\n",
    "                    compaction_data[cf_name].append((start_time_seconds, end_time_seconds, read_rate, write_rate, output_level))\n",
    "                except:\n",
    "                    print(\"Compaction json error\")\n",
    "\n",
    "    # Determine the overall start time\n",
    "    print(f'Exp start time: {experiment_start_time}')\n",
    "\n",
    "    # Convert L0 and Memtable timestamps to seconds since experiment start\n",
    "    l0_timestamps = [(timestamp_micros / 1e6) - experiment_start_time for timestamp_micros, _ in l0_stalls]\n",
    "    l0_rates = [int(rate) for _, rate in l0_stalls]\n",
    "\n",
    "    memtable_timestamps = [(timestamp_micros / 1e6) - experiment_start_time for timestamp_micros, _ in memtable_stalls]\n",
    "    memtable_rates = [int(rate) for _, rate in memtable_stalls]\n",
    "\n",
    "    pending_compaction_timestamps = [(timestamp_micros / 1e6) - experiment_start_time for timestamp_micros, _ in pending_compaction_stalls]\n",
    "    pending_compaction_rates = [int(rate) for _, rate in pending_compaction_stalls]\n",
    "\n",
    "    # Plotting\n",
    "    # plt.figure(figsize=(8, 5))\n",
    "\n",
    "    # Plot L0 and Memtable Stalls\n",
    "    axs[fig_loc[0]].scatter(l0_timestamps, l0_rates, label='L0 Stalls', color='blue', s=10)\n",
    "    axs[fig_loc[0]].scatter(memtable_timestamps, memtable_rates, label='Memtable Stalls', color='purple', s=10)\n",
    "    axs[fig_loc[0]].scatter(pending_compaction_timestamps, pending_compaction_rates, label='Pend Compact Stalls', color='orange', s=10)\n",
    "\n",
    "    # TODO(tgriggs): Add stops to this\n",
    "    with open(output_file, 'a') as outfile:\n",
    "        outfile.write(f\"metric-L0_stalls\\n\")\n",
    "        outfile.write(f\"time_points:{l0_timestamps}\\n\")\n",
    "        outfile.write(f\"data_points:{l0_rates}\\n\")\n",
    "\n",
    "        outfile.write(f\"metric-memtable_stalls\\n\")\n",
    "        outfile.write(f\"time_points:{memtable_timestamps}\\n\")\n",
    "        outfile.write(f\"data_points:{memtable_rates}\\n\")\n",
    "\n",
    "        outfile.write(f\"metric-pending_compaction_stalls\\n\")\n",
    "        outfile.write(f\"time_points:{pending_compaction_timestamps}\\n\")\n",
    "        outfile.write(f\"data_points:{pending_compaction_rates}\\n\")\n",
    "    \n",
    "   \n",
    "    # Plot Flush Events\n",
    "    for cf_name in flush_data:\n",
    "        if cf_name == \"default\":\n",
    "            color = 'red'\n",
    "            label = 'Client 0 Flush'\n",
    "        elif cf_name == 'cf2':\n",
    "            color = 'blue'\n",
    "            label = 'Client 1 Flush'\n",
    "        elif cf_name == 'cf3':\n",
    "            color = 'orange'\n",
    "            label = 'Client 2 Flush'\n",
    "        else:\n",
    "            color = 'green'\n",
    "            label = 'Client 3 Flush'\n",
    "        for start_time, rate, duration in flush_data[cf_name]:\n",
    "            # Determine if this is the first occurrence of this cf_name for labeling\n",
    "            # is_first = flush_data.index((start_time, rate, duration, cf_name)) == 0\n",
    "            is_first = flush_data[cf_name].index((start_time, rate, duration)) == 0\n",
    "            # is_first = compaction_data[cf_name].index((start_time, end_time, read_rate, write_rate, output_level)) == 0\n",
    "\n",
    "            # Plotting with custom dashed line pattern for stripes\n",
    "            axs[fig_loc[0]].hlines(rate, start_time - experiment_start_time, \n",
    "                                (start_time + duration) - experiment_start_time, \n",
    "                                colors=color, linewidth=4, \n",
    "                                linestyles=(0, (5, 10)) if is_first else 'solid', \n",
    "                                label=label if is_first else \"\")\n",
    "\n",
    "    # Plot Compaction Events\n",
    "    for cf_name in compaction_data:\n",
    "        for start_time, end_time, read_rate, write_rate, output_level in compaction_data[cf_name]:\n",
    "            # TODO(tgriggs): CHANGE 3 to ORANGE to match defaults\n",
    "            color = get_compaction_color(cf_name, output_level)\n",
    "            if cf_name == 'default':\n",
    "                label = 'Client 0 Compaction'\n",
    "            else:\n",
    "                label = 'Client ' + cf_name[-1] + ' Compaction'\n",
    "            axs[fig_loc[0]].hlines(write_rate, start_time - experiment_start_time, end_time - experiment_start_time, colors=color, linewidth=2, label=label if compaction_data[cf_name].index((start_time, end_time, read_rate, write_rate, output_level)) == 0 else \"\")\n",
    "            # axs[fig_loc[0]].hlines(read_rate, start_time - experiment_start_time, end_time - experiment_start_time, colors=color, linewidth=2, label=label if compaction_data[cf_name].index((start_time, end_time, read_rate, write_rate, output_level)) == 0 else \"\")\n",
    "    \n",
    "    # memtable_stop_timestamps = [(timestamp_micros / 1e6) - experiment_start_time for timestamp_micros in memtable_stops]\n",
    "    memtable_stops_cf_seen = set()\n",
    "    for i in range(len(memtable_stops)):\n",
    "        memtable_stop_ts, cf_name = memtable_stops[i]\n",
    "        memtable_stop_ts = (memtable_stop_ts / 1e6) - experiment_start_time\n",
    "        if i == 0:\n",
    "        # if cf_name not in memtable_stops_cf_seen:\n",
    "            axs[fig_loc[0]].axvline(x=memtable_stop_ts, color='brown', linestyle='--', linewidth=0.5, label = 'Memtable Stops', alpha=0.5)\n",
    "            memtable_stops_cf_seen.add(cf_name)\n",
    "        else:\n",
    "            axs[fig_loc[0]].axvline(x=memtable_stop_ts, color='brown', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "    with open(output_file, 'a') as outfile:\n",
    "        outfile.write(f\"metric-flush\\n\")\n",
    "        all_cfs = []\n",
    "        all_rates = []\n",
    "        all_start_points = []\n",
    "        all_end_points = []\n",
    "        for cf in flush_data:\n",
    "            for start_time, rate, duration in flush_data[cf]:\n",
    "                all_cfs.append(cf)\n",
    "                all_rates.append(rate)\n",
    "                all_start_points.append(start_time)\n",
    "                all_end_points.append(start_time + duration)\n",
    "        outfile.write(f\"cf_names:{all_cfs}\\n\")\n",
    "        outfile.write(f\"reates:{all_rates}\\n\")\n",
    "        outfile.write(f\"start_points:{[start_time - experiment_start_time for start_time in all_start_points]}\\n\")\n",
    "        outfile.write(f\"end_points:{[end_time - experiment_start_time for end_time in all_end_points]}\\n\")\n",
    "\n",
    "        outfile.write(f\"metric-compaction\\n\")\n",
    "        all_cfs = []\n",
    "        all_read_rates = []\n",
    "        all_write_rates = []\n",
    "        all_start_points = []\n",
    "        all_end_points = []\n",
    "        for cf in compaction_data:\n",
    "            for start_time, end_time, read_rate, write_rate, _ in compaction_data[cf]:\n",
    "                all_read_rates.append(read_rate)\n",
    "                all_write_rates.append(write_rate)\n",
    "                all_start_points.append(start_time)\n",
    "                all_end_points.append(end_time)\n",
    "                all_cfs.append(cf)\n",
    "        outfile.write(f\"cf_names:{all_cfs}\\n\")\n",
    "        outfile.write(f\"read_rates:{all_read_rates}\\n\")\n",
    "        outfile.write(f\"write_rates:{all_write_rates}\\n\")\n",
    "        outfile.write(f\"start_points:{[start_time - experiment_start_time for start_time in all_start_points]}\\n\")\n",
    "        outfile.write(f\"end_points:{[end_time - experiment_start_time for end_time in all_end_points]}\\n\")\n",
    "        \n",
    "        outfile.write(f\"metric-memtable_stops\\n\")\n",
    "        outfile.write(f\"time_points:{memtable_stops}\\n\")\n",
    "        \n",
    "\n",
    "    axs[fig_loc[0]].set_title('Database Operations Over Time')\n",
    "    axs[fig_loc[0]].set_xlabel('Time (seconds since start of experiment)')\n",
    "    axs[fig_loc[0]].set_ylabel('MB/s')\n",
    "    axs[fig_loc[0]].set_xlim(xlim)\n",
    "    # axs[fig_loc[0]].set_ylim(0, 450)\n",
    "    axs[fig_loc[0]].legend()\n",
    "    axs[fig_loc[0]].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall throughputs (client + system)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "max_read_tput = 700\n",
    "max_read_iops = 180000\n",
    "max_write_tput = 400\n",
    "max_write_iops = 100000\n",
    "\n",
    "def timestamp_to_seconds(timestamp_str):\n",
    "  timestamp = datetime.strptime(timestamp_str.rstrip(), '%Y-%m-%d %H:%M:%S.%f')\n",
    "  epoch = datetime(1970, 1, 1)\n",
    "  return (timestamp - epoch).total_seconds()\n",
    "\n",
    "def plot_overall_tputs(output_file, axs, start_time_shift, xlim, fig_loc):\n",
    "  df_from_csv = pd.read_csv(\"iostat_results.csv\")\n",
    "\n",
    "  # print(f\"Timestamp: {timestamp_to_seconds(df_from_csv['Timestamp'][0])}\")\n",
    "\n",
    "  time_seconds = np.arange(len(df_from_csv))\n",
    "  time_seconds = [x + start_time_shift for x in time_seconds]\n",
    "\n",
    "  # Plotting\n",
    "  # axs[fig_loc[0]].figure(figsize=(10, 6))\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"rMB/s\"], label='Read MB/s', marker='o', color='tab:green')\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"wMB/s\"], label='Write MB/s', marker='o', color='tab:red')\n",
    "\n",
    "  with open(output_file, 'a') as outfile:\n",
    "    outfile.write(f\"metric-rMB_rate\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"rMB/s\"].tolist()}\\n\")\n",
    "\n",
    "    outfile.write(f\"metric-wMB_rate\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"wMB/s\"].tolist()}\\n\")\n",
    "\n",
    "  axs[fig_loc[0]].set_title('SSD Throughput Over Time')\n",
    "  axs[fig_loc[0]].set_xlabel('Time (s)')\n",
    "  axs[fig_loc[0]].set_ylabel('MB/s')\n",
    "  axs[fig_loc[0]].set_xlim(xlim)\n",
    "  # axs[fig_loc[0]].set_ylim(0, 520)\n",
    "  axs[fig_loc[0]].legend(loc='upper left')\n",
    "  axs[fig_loc[0]].grid(True)\n",
    "\n",
    "  # # Creating a second y-axis\n",
    "  # ax2 = axs[fig_loc[0]].twinx()\n",
    "  # # Plotting on the secondary y-axis\n",
    "  # ax2.plot(time_seconds, df_from_csv[\"rMB/s\"]/max_read_tput, label='Read Util', marker='x', linestyle='--', color='tab:green')\n",
    "  # ax2.plot(time_seconds, df_from_csv[\"wMB/s\"]/max_write_tput, label='Write Util', marker='+', linestyle='--', color='tab:red')\n",
    "  # ax2.set_ylabel('Utilization (based on tput)')\n",
    "  # ax2.legend(loc='upper right')\n",
    "  # ax2.set_ylim(0,1)\n",
    "\n",
    "  # Adjust the right margin to accommodate the second y-axis legend\n",
    "  plt.subplots_adjust(right=0.85)\n",
    "\n",
    "  with open(output_file, 'a') as outfile:\n",
    "    outfile.write(f\"metric-rMB_util\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{(df_from_csv[\"rMB/s\"]/max_read_tput).tolist()}\\n\")\n",
    "\n",
    "    outfile.write(f\"metric-wMB_util\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{(df_from_csv[\"wMB/s\"]/max_write_tput).tolist()}\\n\")\n",
    "\n",
    "def plot_overall_iops(output_file, axs, start_time_shift, xlim, fig_loc):\n",
    "  df_from_csv = pd.read_csv(\"iostat_results.csv\")\n",
    "  time_seconds = np.arange(len(df_from_csv))\n",
    "  time_seconds = [x + start_time_shift for x in time_seconds]\n",
    "\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"r/s\"], label='Read IOPS', marker='o', color='tab:green')\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"w/s\"], label='Write IOPS ', marker='o', color='tab:red')\n",
    "\n",
    "  with open(output_file, 'a') as outfile:\n",
    "    outfile.write(f\"metric-rIOP_rate\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"r/s\"].tolist()}\\n\")\n",
    "\n",
    "    outfile.write(f\"metric-wIOP_rate\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"w/s\"].tolist()}\\n\")\n",
    "\n",
    "  axs[fig_loc[0]].set_title('SSD IOPS Over Time')\n",
    "  axs[fig_loc[0]].set_xlabel('Time (s)')\n",
    "  axs[fig_loc[0]].set_ylabel('IOPS')\n",
    "  axs[fig_loc[0]].set_xlim(xlim)\n",
    "  # axs[fig_loc[0]].set_ylim(0,16)\n",
    "  axs[fig_loc[0]].legend(loc='upper right')\n",
    "  axs[fig_loc[0]].grid(True)\n",
    "\n",
    "  # Creating a second y-axis\n",
    "  ax2 = axs[fig_loc[0]].twinx()\n",
    "  # Plotting on the secondary y-axis\n",
    "  ax2.plot(time_seconds, df_from_csv[\"r/s\"]/max_read_iops, label='Read IOPS Util', marker='x', linestyle='--', color='tab:green')\n",
    "  ax2.plot(time_seconds, df_from_csv[\"w/s\"]/max_write_iops, label='Write IOPS Util', marker='+', linestyle='--', color='tab:red')\n",
    "  ax2.set_ylabel('Utilization (based on iops)')\n",
    "  ax2.legend(loc='upper right')\n",
    "  ax2.set_ylim(0,1)\n",
    "\n",
    "  # Adjust the right margin to accommodate the second y-axis legend\n",
    "  plt.subplots_adjust(right=0.85)\n",
    "\n",
    "  with open(output_file, 'a') as outfile:\n",
    "    outfile.write(f\"metric-rIOP_util\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{(df_from_csv[\"r/s\"]/max_read_iops).tolist()}\\n\")\n",
    "\n",
    "    outfile.write(f\"metric-wIOP_util\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{(df_from_csv[\"w/s\"]/max_write_iops).tolist()}\\n\")\n",
    "\n",
    "def plot_io_waittimes(output_file, axs, start_time_shift, xlim, fig_loc):\n",
    "  df_from_csv = pd.read_csv(\"iostat_results.csv\")\n",
    "  time_seconds = np.arange(len(df_from_csv))\n",
    "  time_seconds = [x + start_time_shift for x in time_seconds]\n",
    "\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"r_await\"], label='Read Await (per req)', marker='o', color='tab:green')\n",
    "  # axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"w_await\"], label='Write Await (per req)', marker='o', color='tab:red')\n",
    "\n",
    "  axs[fig_loc[0]].set_title('IO Wait Times (queueing + servicing)')\n",
    "  axs[fig_loc[0]].set_xlabel('Time (s)')\n",
    "  axs[fig_loc[0]].set_ylabel('Wait Time (ms)')\n",
    "  axs[fig_loc[0]].set_xlim(xlim)\n",
    "  # axs[fig_loc[0]].set_ylim(0,1)\n",
    "  axs[fig_loc[0]].legend(loc='upper left')\n",
    "  axs[fig_loc[0]].grid(True)\n",
    "\n",
    "  ax2 = axs[fig_loc[0]].twinx()\n",
    "  ax2.plot(time_seconds, [df_from_csv[\"r_await\"][i] / df_from_csv[\"rareq-sz\"][i] if df_from_csv[\"rareq-sz\"][i] > 0 else df_from_csv[\"r_await\"][i] for i in range(len(df_from_csv[\"r_await\"]))], label='Read Await (per KB)', marker='x', color='tab:green')\n",
    "  # ax2.plot(time_seconds, [df_from_csv[\"w_await\"][i] / df_from_csv[\"wareq-sz\"][i] if df_from_csv[\"wareq-sz\"][i] > 0 else df_from_csv[\"w_await\"][i]   for i in range(len(df_from_csv[\"w_await\"]))], label='Write Await (per KB)', marker='x', color='tab:red')\n",
    "\n",
    "  ax2.set_ylabel('IO Wait Times per KB')\n",
    "  ax2.legend(loc='upper right')\n",
    "  plt.subplots_adjust(right=0.85)\n",
    "\n",
    "  with open(output_file, 'a') as outfile:\n",
    "    outfile.write(f\"metric-r_await\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"r_await\"].tolist()}\\n\")\n",
    "\n",
    "    outfile.write(f\"metric-r_await_per_kb\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{[df_from_csv[\"r_await\"][i] / df_from_csv[\"rareq-sz\"][i] if df_from_csv[\"rareq-sz\"][i] > 0 else df_from_csv[\"r_await\"][i] for i in range(len(df_from_csv[\"r_await\"]))]}\\n\")\n",
    "\n",
    "def plot_io_reqsize(output_file, axs, start_time_shift, xlim, fig_loc):\n",
    "  df_from_csv = pd.read_csv(\"iostat_results.csv\")\n",
    "  time_seconds = np.arange(len(df_from_csv))\n",
    "  time_seconds = [x + start_time_shift for x in time_seconds]\n",
    "\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"rareq-sz\"], label='Avg Read Size', marker='o', color='tab:green')\n",
    "  axs[fig_loc[0]].plot(time_seconds, df_from_csv[\"wareq-sz\"], label='Avg Write Size', marker='o', color='tab:red')\n",
    "\n",
    "  axs[fig_loc[0]].set_title('Avg IO Sizes')\n",
    "  axs[fig_loc[0]].set_xlabel('Time (s)')\n",
    "  axs[fig_loc[0]].set_ylabel('Size (KB)')\n",
    "  axs[fig_loc[0]].set_xlim(xlim)\n",
    "  # axs[fig_loc[0]].set_ylim(0,1)\n",
    "  axs[fig_loc[0]].legend(loc='upper right')\n",
    "  axs[fig_loc[0]].grid(True)\n",
    "\n",
    "  with open(output_file, 'a') as outfile:\n",
    "    outfile.write(f\"metric-r_size\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"rareq-sz\"].tolist()}\\n\")\n",
    "\n",
    "    outfile.write(f\"metric-w_size\\n\")\n",
    "    outfile.write(f\"time_points:{time_seconds}\\n\")\n",
    "    outfile.write(f\"data_points:{df_from_csv[\"wareq-sz\"].tolist()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def plot_cpu_util(output_file, axs, start_time_shift, xlim, fig_loc):\n",
    "    df = pd.read_csv(\"mpstat_results.csv\")\n",
    "    \n",
    "    # Assuming core values are strings and stripping any potential whitespace\n",
    "    df['core'] = df['core'].map(lambda x: x.strip())\n",
    "    df_filtered = df[df['core'].isin(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15'])].copy()\n",
    "\n",
    "    # Convert metrics to float\n",
    "    df_filtered['usr'] = df_filtered['usr'].map(lambda x: float(x))\n",
    "    df_filtered['sys'] = df_filtered['sys'].map(lambda x: float(x))\n",
    "    df_filtered['iowait'] = df_filtered['iowait'].map(lambda x: float(x))\n",
    "    df_filtered['soft'] = df_filtered['soft'].map(lambda x: float(x))\n",
    "    df_filtered['idle'] = df_filtered['idle'].map(lambda x: float(x))\n",
    "\n",
    "    # Calculate the sum of iowait and idle for each row\n",
    "    df_filtered['sum_iowait_idle'] = df_filtered['iowait'] + df_filtered['idle']\n",
    "    \n",
    "    # For plotting purposes, assuming each row represents a sequential time unit\n",
    "    time_seconds = np.arange(len(df_filtered))\n",
    "    time_seconds = [x + start_time_shift for x in time_seconds]\n",
    "\n",
    "    # Group by core and plot each group\n",
    "    for core, group in df_filtered.groupby('core'):\n",
    "        # Calculate the index for each group based on its length\n",
    "        group_index = list(np.arange(len(group)))\n",
    "        axs[fig_loc[0]].plot(group_index, [100 - x for x in group['sum_iowait_idle']], label=f'Core {core} idle+iowait')\n",
    "        with open(output_file, 'a') as outfile:\n",
    "            outfile.write(f\"metric-cpu_util:core-{core}\\n\")\n",
    "            outfile.write(f\"time_points:{group_index}\\n\")\n",
    "            outfile.write(f\"data_points:{[100 - x for x in group['sum_iowait_idle']]}\\n\")\n",
    "\n",
    "    axs[fig_loc[0]].set_title('CPU Utilization (all but iowait and idle)')\n",
    "    axs[fig_loc[0]].set_xlabel('Time (seconds)')\n",
    "    axs[fig_loc[0]].legend(loc='upper right')\n",
    "    axs[fig_loc[0]].grid(True)\n",
    "    axs[fig_loc[0]].set_xlim(xlim)\n",
    "    axs[fig_loc[0]].set_ylim(-2, 102)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def plot_rsched_stats(output_file, axs, start_time_s, xlim, fig_loc):\n",
    "    start_time_us = start_time_s * 1e6\n",
    "    # Initialize lists to store the data for each client\n",
    "    timestamps = defaultdict(list)  # Dictionary to store timestamps for each client\n",
    "    memtables_data = defaultdict(lambda: ([], []))  # (list of values before '-', list of values after '-')\n",
    "    writes_data = defaultdict(list)\n",
    "    reads_data = defaultdict(list)\n",
    "\n",
    "    # Function to parse a line and extract the data\n",
    "    def parse_csv_row(row):\n",
    "        timestamp, client_id, write_rate_limit_kbs, read_rate_limit_kbs, write_buffer_size_kb, max_write_buffer_number = row\n",
    "        timestamp = (float(timestamp) - start_time_us) / 1e6\n",
    "        client_id = int(client_id)\n",
    "        timestamps[client_id].append(timestamp)\n",
    "        memtables_data[client_id][0].append(int(max_write_buffer_number))\n",
    "        memtables_data[client_id][1].append(int(write_buffer_size_kb) / 1024)\n",
    "        writes_data[client_id].append(int(write_rate_limit_kbs) / 1024)\n",
    "        reads_data[client_id].append(int(read_rate_limit_kbs) / 1024)\n",
    "\n",
    "    # Read the CSV file\n",
    "    csv_file = 'logs/resource_shares.log'  # Replace with your CSV file name\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            parse_csv_row(row)\n",
    "\n",
    "    # Helper function to calculate total allocations\n",
    "    def calculate_total(data):\n",
    "        max_length = max(len(lst) for lst in data.values())\n",
    "        total_allocation = [0] * max_length\n",
    "        for lst in data.values():\n",
    "            for i in range(len(lst)):\n",
    "                total_allocation[i] += lst[i]\n",
    "        return total_allocation\n",
    "\n",
    "    # Calculate total allocations\n",
    "    if len(writes_data) == 0 and len(reads_data) == 0 and len(memtables_data) == 0:\n",
    "        return\n",
    "\n",
    "    # Plot the memtables data with two different axes\n",
    "    ax1 = axs[fig_loc[0]]\n",
    "    min_length = min(len(data[0]) for data in memtables_data.values())\n",
    "    cumulative_data = np.zeros(min_length)\n",
    "\n",
    "    for client in sorted(memtables_data.keys()):\n",
    "        time_axis = timestamps[client][:min_length]\n",
    "        current_data = np.array(memtables_data[client][0][:min_length])\n",
    "        new_cumulative_data = cumulative_data + current_data\n",
    "        \n",
    "        ax1.plot(time_axis, new_cumulative_data, label=f'Client {client} #table')\n",
    "        ax1.fill_between(time_axis, cumulative_data, new_cumulative_data, alpha=0.3)\n",
    "        \n",
    "        cumulative_data = new_cumulative_data\n",
    "\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Max num memtables')\n",
    "    ax1.set_title('Memtable Limits')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.set_xlim(xlim)\n",
    "\n",
    "    # Plot the writes data for all clients\n",
    "    ax2 = axs[fig_loc[1]]\n",
    "    min_length = min(len(writes_data[client]) for client in writes_data)\n",
    "    cumulative_data = np.zeros(min_length)\n",
    "\n",
    "    for client in sorted(writes_data.keys()):\n",
    "        time_axis = timestamps[client][:min_length]\n",
    "        current_data = np.array(writes_data[client][:min_length])\n",
    "        new_cumulative_data = cumulative_data + current_data\n",
    "        \n",
    "        ax2.plot(time_axis, new_cumulative_data, label=f'Client {client}')\n",
    "        ax2.fill_between(time_axis, cumulative_data, new_cumulative_data, alpha=0.3)\n",
    "        \n",
    "        cumulative_data = new_cumulative_data\n",
    "\n",
    "    ax2.set_title('IO Write Rate Limit')\n",
    "    ax2.set_xlabel('Time (s)')\n",
    "    ax2.set_ylabel('MB/s')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlim(xlim)\n",
    "    # ax2.set_ylim(5, 300)\n",
    "\n",
    "    # Plot the reads data for all clients\n",
    "    ax3 = axs[fig_loc[2]]\n",
    "    min_length = min(len(reads_data[client]) for client in reads_data)\n",
    "    cumulative_data = np.zeros(min_length)\n",
    "\n",
    "    for client in sorted(reads_data.keys()):\n",
    "        time_axis = timestamps[client][:min_length]\n",
    "        current_data = np.array(reads_data[client][:min_length])\n",
    "        new_cumulative_data = cumulative_data + current_data\n",
    "        \n",
    "        ax3.plot(time_axis, new_cumulative_data, label=f'Client {client}')\n",
    "        ax3.fill_between(time_axis, cumulative_data, new_cumulative_data, alpha=0.3)\n",
    "        \n",
    "        cumulative_data = new_cumulative_data\n",
    "\n",
    "    ax3.set_title('IO Read Rate Limit')\n",
    "    ax3.set_xlabel('Time (s)')\n",
    "    ax3.set_ylabel('MB/s')\n",
    "    ax3.legend()\n",
    "    ax3.set_xlim(xlim)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(output_file)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'logs/client_stats.log'  # Replace with your CSV file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filter for op_type \"QUEUE\"\n",
    "queue_data = data[data['op_type'] == 'QUEUE'].copy()\n",
    "\n",
    "# Exclude client IDs '3' and '4'\n",
    "# queue_data = queue_data[~queue_data['client_id'].isin([3, 4])]\n",
    "\n",
    "# Adjust timestamps relative to the starting timestamp\n",
    "queue_data['timestamp'] = (queue_data['timestamp'] - queue_data['timestamp'].min()) / 1000\n",
    "\n",
    "# Create separate DataFrames for 50p and 99p latency\n",
    "latency_50p = queue_data.pivot(index='timestamp', columns='client_id', values='50p')\n",
    "latency_99p = queue_data.pivot(index='timestamp', columns='client_id', values='99p')\n",
    "\n",
    "# Plot 50p latency\n",
    "plt.figure(figsize=(12, 4))\n",
    "for client_id in latency_50p.columns:\n",
    "    plt.plot(latency_50p.index, latency_50p[client_id], label=f'Client {client_id}')\n",
    "plt.title('P50 Queuing Latency Over Time')\n",
    "plt.xlabel('Relative Timestamp')\n",
    "plt.ylabel('50th Percentile Latency (ms)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 99p latency\n",
    "plt.figure(figsize=(12, 4))\n",
    "for client_id in latency_99p.columns:\n",
    "    plt.plot(latency_99p.index, latency_99p[client_id], label=f'Client {client_id}')\n",
    "plt.title('P99 Queuing Latency Over Time')\n",
    "plt.xlabel('Relative Timestamp')\n",
    "plt.ylabel('99th Percentile Latency (ms)')\n",
    "plt.legend()\n",
    "# plt.xlim(280, 340)\n",
    "# plt.ylim(0, 4000)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def plot_wbm_data(file_path):\n",
    "    # Initialize a nested dictionary to store data for each client id\n",
    "    client_data = defaultdict(lambda: {'global': [], 'steady': []})\n",
    "\n",
    "    # Read the file and parse relevant lines\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"wbm\"):\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) == 7 and parts[3] == 'res':\n",
    "                    # Process 'res' line with steady or global\n",
    "                    try:\n",
    "                        _, timestamp, client_id, operation, operation_size, current_value, steady_or_global = parts\n",
    "                        timestamp = int(timestamp) / 1000  # Convert to seconds\n",
    "                        client_id = int(client_id)\n",
    "                        current_value = int(current_value) / (1024 * 1024)  # Convert to MB\n",
    "                        steady_or_global = steady_or_global.strip()\n",
    "                        if steady_or_global in ['steady', 'global']:\n",
    "                            client_data[client_id][steady_or_global].append((timestamp, current_value))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                elif len(parts) == 8 and parts[3] == 'free':\n",
    "                    # Process 'free' line with global:XXX and steady:YYY\n",
    "                    try:\n",
    "                        _, timestamp, client_id, operation, operation_size, current_value, global_str, steady_str = parts\n",
    "                        timestamp = int(timestamp) / 1000\n",
    "                        client_id = int(client_id)\n",
    "                        # Extract the global and steady values\n",
    "                        global_value = int(global_str.split(':')[1]) / (1024 * 1024)  # Convert to MB\n",
    "                        steady_value = int(steady_str.split(':')[1]) / (1024 * 1024)  # Convert to MB\n",
    "                        # Store the values\n",
    "                        client_data[client_id]['global'].append((timestamp, global_value))\n",
    "                        client_data[client_id]['steady'].append((timestamp, steady_value))\n",
    "                    except (ValueError, IndexError):\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # Determine the start time of the experiment\n",
    "    start_time = min(\n",
    "        min((timestamps[0][0] for timestamps in client.values() if len(timestamps) > 0), default=float('inf'))\n",
    "        for client in client_data.values()\n",
    "    )\n",
    "\n",
    "    # Adjust timestamps and convert to numpy arrays\n",
    "    for client_id in client_data:\n",
    "        for key in ['global', 'steady']:\n",
    "            values = client_data[client_id][key]\n",
    "            adjusted_values = [(timestamp - start_time, value) for timestamp, value in values]\n",
    "            client_data[client_id][key] = np.array(adjusted_values)\n",
    "\n",
    "    # Collect all timestamps\n",
    "    all_timestamps = sorted(set(\n",
    "        timestamp\n",
    "        for client_values in client_data.values()\n",
    "        for key in ['global', 'steady']\n",
    "        for timestamp, _ in client_values[key]\n",
    "    ))\n",
    "\n",
    "    # Initialize latest values for each client and key\n",
    "    latest_values = {client_id: {'global': 0, 'steady': 0} for client_id in client_data}\n",
    "    total_usage_values = []\n",
    "    sum_steady_values = []\n",
    "    sum_global_values = []\n",
    "\n",
    "    # Calculate per-client total usage and total usages\n",
    "    for ts in all_timestamps:\n",
    "        total_global = 0\n",
    "        total_steady = 0\n",
    "        total_usage = 0\n",
    "        per_client_total_usage = {}\n",
    "        for client_id in client_data:\n",
    "            total_client_usage = 0\n",
    "            for key in ['global', 'steady']:\n",
    "                values = client_data[client_id][key]\n",
    "                if len(values) == 0:\n",
    "                    continue\n",
    "                mask = values[:, 0] <= ts\n",
    "                if np.any(mask):\n",
    "                    latest_values[client_id][key] = values[mask, 1][-1]\n",
    "                # Sum up the latest values\n",
    "            # Sum per-client total usage\n",
    "            total_client_usage = latest_values[client_id]['global'] + latest_values[client_id]['steady']\n",
    "            per_client_total_usage[client_id] = total_client_usage\n",
    "            total_usage += total_client_usage\n",
    "            total_global += latest_values[client_id]['global']\n",
    "            total_steady += latest_values[client_id]['steady']\n",
    "        total_usage_values.append((ts, total_usage))\n",
    "        sum_steady_values.append((ts, total_steady))\n",
    "        sum_global_values.append((ts, total_global))\n",
    "\n",
    "    # Convert total usage values to numpy arrays\n",
    "    total_usage_values = np.array(total_usage_values)\n",
    "    sum_steady_values = np.array(sum_steady_values)\n",
    "    sum_global_values = np.array(sum_global_values)\n",
    "\n",
    "    # Prepare per-client total usage arrays for plotting\n",
    "    client_total_usage = {}\n",
    "    for client_id in client_data:\n",
    "        timestamps = []\n",
    "        usage_values = []\n",
    "        latest_values = {'global': 0, 'steady': 0}\n",
    "        for ts in all_timestamps:\n",
    "            for key in ['global', 'steady']:\n",
    "                values = client_data[client_id][key]\n",
    "                if len(values) == 0:\n",
    "                    continue\n",
    "                mask = values[:, 0] <= ts\n",
    "                if np.any(mask):\n",
    "                    latest_values[key] = values[mask, 1][-1]\n",
    "            total_client_usage = latest_values['global'] + latest_values['steady']\n",
    "            timestamps.append(ts)\n",
    "            usage_values.append(total_client_usage)\n",
    "        client_total_usage[client_id] = np.array([timestamps, usage_values])\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot per-client total usage\n",
    "    for client_id, data in client_total_usage.items():\n",
    "        plt.plot(data[0], data[1], label=f\"Client {client_id} Total Usage\")\n",
    "\n",
    "    # Plot summed lines\n",
    "    plt.plot(total_usage_values[:, 0], total_usage_values[:, 1], label=\"All Clients Total Usage\", color=\"black\", linestyle=\"-\", linewidth=2)\n",
    "    plt.plot(sum_steady_values[:, 0], sum_steady_values[:, 1], label=\"Total Steady Usage\", color=\"green\", linestyle=\"--\", linewidth=2)\n",
    "    plt.plot(sum_global_values[:, 0], sum_global_values[:, 1], label=\"Total Global Usage\", color=\"blue\", linestyle=\":\", linewidth=2)\n",
    "\n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel(\"Time Since Start (s)\")\n",
    "    plt.ylabel(\"Usage (MB)\")\n",
    "    plt.title(\"Memtable Shares Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    # plt.ylim(0, 256)\n",
    "    # plt.xlim(0, 25)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the path to your file\n",
    "file_path = \"logs/memtable_stats.txt\"  # Replace with your file's path\n",
    "#plot_wbm_data(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration: Operation sizes (in KB)\n",
    "READ_SIZE_KB = 1  # Size of a single READ operation\n",
    "INSERT_SIZE_KB = 1  # Size of a single INSERT operation\n",
    "INSERT_BATCH_SIZE_KB = 100  # Size of a single INSERT_BATCH operation\n",
    "\n",
    "# Convert sizes to MB for throughput calculations\n",
    "READ_SIZE_MB = READ_SIZE_KB / 1024\n",
    "INSERT_SIZE_MB = INSERT_SIZE_KB / 1024\n",
    "INSERT_BATCH_SIZE_MB = INSERT_BATCH_SIZE_KB / 1024\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('logs/client_stats.log')\n",
    "\n",
    "# Convert timestamp to relative times (in seconds)\n",
    "df['relative_time_ms'] = df['timestamp'] - df['timestamp'].min()\n",
    "df['relative_time_s'] = df['relative_time_ms'] / 1000.0\n",
    "\n",
    "# Sort data and calculate time differences\n",
    "df = df.sort_values(by=['client_id', 'op_type', 'timestamp'])\n",
    "df['time_diff_s'] = df.groupby(['client_id', 'op_type'])['relative_time_s'].diff()\n",
    "\n",
    "# Calculate throughput (ops/sec) and convert to MB/s\n",
    "df['throughput'] = df['count'] / df['time_diff_s']\n",
    "df['throughput_mb_s'] = 0.0  # initialize as float\n",
    "\n",
    "df.loc[df['op_type'] == 'READ', 'throughput_mb_s'] = df['throughput'] * READ_SIZE_MB\n",
    "df.loc[df['op_type'] == 'INSERT', 'throughput_mb_s'] = df['throughput'] * INSERT_SIZE_MB\n",
    "df.loc[df['op_type'] == 'INSERT_BATCH', 'throughput_mb_s'] = df['throughput'] * INSERT_BATCH_SIZE_MB\n",
    "\n",
    "# Drop NaN throughput rows\n",
    "df_throughput = df.dropna(subset=['throughput_mb_s'])\n",
    "\n",
    "# Separate dataframes for different operation categories\n",
    "df_insert = df_throughput[df_throughput['op_type'] == 'INSERT']\n",
    "df_insert_batch = df_throughput[df_throughput['op_type'] == 'INSERT_BATCH']\n",
    "df_queue_latency = df[df['op_type'] == 'QUEUE']\n",
    "df_insert_latency = df[df['op_type'] == 'INSERT']\n",
    "\n",
    "# Create a copy of df_read to avoid SettingWithCopyWarning\n",
    "df_read = df_read.copy()\n",
    "\n",
    "# Calculate user cache total and hit rate\n",
    "df_read['user_cache_total'] = df_read['user_cache_hits'] + df_read['user_cache_misses']\n",
    "df_read['user_cache_hit_rate'] = np.where(\n",
    "    df_read['user_cache_total'] > 0,\n",
    "    (df_read['user_cache_hits'] / df_read['user_cache_total']) * 100.0,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Prepare data for user_cache_usage stacked area plot\n",
    "df_cache_usage = df_read[['relative_time_s', 'client_id', 'user_cache_usage']].dropna()\n",
    "df_cache_usage_pivot = df_cache_usage.pivot_table(\n",
    "    index='relative_time_s',\n",
    "    columns='client_id',\n",
    "    values='user_cache_usage',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Create figure and axes\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 10), sharex=True)\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "\n",
    "# 2. Per-Client WRITE Throughput (INSERT & INSERT_BATCH)\n",
    "ax2 = axes[0]\n",
    "for client_id, grp in df_insert.groupby('client_id'):\n",
    "    ax2.plot(grp['relative_time_s'], grp['throughput_mb_s'], label=f'Client {client_id} INSERT')\n",
    "for client_id, grp in df_insert_batch.groupby('client_id'):\n",
    "    ax2.plot(grp['relative_time_s'], grp['throughput_mb_s'], label=f'Client {client_id} INSERT_BATCH')\n",
    "ax2.set_title('Per-Client WRITE Throughput (INSERT & INSERT_BATCH)')\n",
    "ax2.set_ylabel('Throughput (MB/s)')\n",
    "# ax2.legend(loc='upper right')\n",
    "\n",
    "\n",
    "# 5. P99 INSERT Latency\n",
    "ax5 = axes[1]\n",
    "for client_id, grp in df_insert_latency.groupby('client_id'):\n",
    "    ax5.plot(grp['relative_time_s'], grp['99p'], label=f'Client {client_id}')\n",
    "ax5.set_title('Per-Client INSERT P99 Latency')\n",
    "ax5.set_ylabel('Latency (ms)')\n",
    "# ax5.legend(loc='upper right')\n",
    "\n",
    "# 6. P99 QUEUE Latency\n",
    "ax6 = axes[2]\n",
    "for client_id, grp in df_queue_latency.groupby('client_id'):\n",
    "    ax6.plot(grp['relative_time_s'], grp['99p'], label=f'Client {client_id}')\n",
    "ax6.set_title('Per-Client QUEUE P99 Latency')\n",
    "ax6.set_ylabel('Latency (ms)')\n",
    "# ax6.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def generate_plots(xlim, output_file):\n",
    "  _, axs = plt.subplots(13, 1, figsize=(12, 50))\n",
    "  plt.subplots_adjust(hspace=0.4)\n",
    "  start_time_s = plot_client_results(output_file, axs, xlim)\n",
    "  plot_rocksdb_events(output_file, axs, start_time_s, xlim, (3,0))\n",
    "  #plot_memtable_stats(output_file, axs, start_time_s, xlim, (4,0))\n",
    "\n",
    "  start_time_shift = 0\n",
    "  plot_overall_tputs(output_file, axs, start_time_shift, xlim, (5,0))\n",
    "  plot_overall_iops(output_file, axs, start_time_shift, xlim, (6,0))\n",
    "  plot_io_waittimes(output_file, axs, start_time_shift, xlim, (7,0))\n",
    "  plot_io_reqsize(output_file, axs, start_time_shift, xlim, (8,0))\n",
    "  plot_cpu_util(output_file, axs, start_time_shift, xlim, (9,0))\n",
    "\n",
    "  # plot_level_stats(output_file, axs, (10, 0))\n",
    "  # plot_rsched_stats(output_file, axs, start_time_s, xlim, (10, 11, 12))\n",
    "  plt.show()\n",
    "\n",
    "xlim = (0, 60)\n",
    "output_file = \"results/timeseries_\" + str(int(time.time())) + \".txt\"\n",
    "generate_plots(xlim, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"logs/WAL_logs.csv\"  # Replace with your actual path\n",
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "df.columns = ['operation', 'time_in_ms', 'current_wal_bytes', 'max_wal_bytes', 'switch_number']\n",
    "\n",
    "# Convert time to seconds for easier visualization\n",
    "df['time_in_s'] = (df['time_in_ms'] - df['time_in_ms'].min()) / 1000\n",
    "\n",
    "# Filter out switch operations separately for markers\n",
    "switch_start = df[df['operation'] == 'switch_start']\n",
    "switch_end = df[df['operation'] == 'switch_end']\n",
    "\n",
    "# Filter only write operations for the main graph\n",
    "write_df = df[df['operation'] == 'write']\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot WAL usage over time\n",
    "ax.plot(write_df['time_in_s'], write_df['current_wal_bytes'] / (1024 * 1024), label='WAL Usage (MB)', color='blue')\n",
    "\n",
    "# Add switch start and end markers\n",
    "ax.scatter(switch_start['time_in_s'], [0] * len(switch_start), color='red', label='Switch Start', marker='^', zorder=3)\n",
    "ax.scatter(switch_end['time_in_s'], [0] * len(switch_end), color='green', label='Switch End', marker='v', zorder=3)\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel(\"Time Since Start (s)\")\n",
    "ax.set_ylabel(\"WAL Usage (MB)\")\n",
    "ax.set_title(\"WAL Data Usage and Switch Operations Over Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
